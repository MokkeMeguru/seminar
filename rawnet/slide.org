#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: 高速に音声分析合成を行う RawNet について読む
#+date: <2019-03-17 Sun>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 音声分析合成ってなぁに？
  音声分析合成というのは、何らかの音声を合成して、それを分析し何らかのデータにして、それを複合して元の音声を生成する、という研究分野です。
  
  ある意味では音声合成（Text-to-speech）はこの一分野としてみなすことができるかもしれません。

  但し本研究はテキストから音声を合成することは出来ないと思われます。（多分テキストから潜在表現への変換をDNNで繋げればある程度可能だと思われますが、Google 音声合成API使えばよくね？と指摘されて [研究資源が降ってこないことが目に見えている](https://qiita.com/MeguruMokke/items/de3c0665ec2fb5ba2110) のでやりません）

* 音声分析合成の関連研究はどんなものがあるの？
  日本語でかなり有名なものとして、[WORLD](https://www.jstage.jst.go.jp/article/transinf/E99.D/7/E99.D_2015EDP7457/_pdf/-char/en)というものがあります。
  このあたり [1](http://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/) [2](https://qiita.com/misumi3104/items/de49e591a72b781c0521) が参考資料としてよいでしょう。
  ここにはないものとして、[LPCNet](https://arxiv.org/pdf/1810.11846.pdf) というものがあり、論文では触れられていますが、こちらについてはそのうち書きます。（~~[Mozilla](https://github.com/mozilla/LPCNet)あたりの研究で、それなりのネームバリューがあります~~）
  
* RawNet の概要は何？ 
  [RawNet](https://arxiv.org/pdf/1904.05351.pdf) は 深層学習を用いた 高速な Vocoder です。Vocoder とは何かと言われると日本語でうまいこと表現する語が思いつきませんが、音声合成するもの、と思っていただければ大丈夫でしょう。

  ちょっと論文の概要を翻訳してみましょう。
  
  >>> 昨今、深層学習を用いた vocoder は高品質の音声を生成することが可能であることがわかってきました。これらのモデルはおおよそメルスペクトラムといったスペクトル特性を用いてサンプルデータを生成します。しかし、それらの特性は人間の知識を元にしてできた（手打ちで作られた）プロセスを含んだ音声分析モジュールを用いて抽出されています。

  つまり、昨今の深層学習を用いた vocoder は学習データを作成する際に人手を使っておりコストがかかってしまっていることを指摘しています。

  スペクトル特性を用いてサンプルデータを生成する大きな利点としては、今の所こちらのほうが速度面で有利な結果が出ていることが挙げられるでしょう。なぜなら音声を直接生成しようとすると、何ミリ秒で１サンプルといった物凄い勢いでデータを推論する機構が必要になるためです。音声サンプリングそんなにたくさんなくても音声再生できるやろ？と思う方もいますが、これはシャノンの定理あたりを勉強するとなんとなくわかると思います。

  >>> 本研究では RawNet と呼ばれるモデルを提案します。これは真に end-to-end な vocoder で、信号のより高次元な表現を学習するために coder ネットワークを使用し、そのサンプルごとに音声サンプルを生成する autoregressive voder ネットワークを使用しています。

   vocoder とは coder と voder で構成されており、 coder は音声を符号化する機能を持っており、voder はそれを複合する機能を持っていると考えてください。このあたりは mp3 のような codec を想定してもらうとわかりやすいでしょう。mp3 では入力される音声をルールに従って圧縮したバイナリデータに変換し、これを同じルールに従って複合することで音声として我々が聞き取ります。
   
   autoregressive ネットワークとは、日本語では自己回帰ネットワークとも言われ、例えば RNN （recurent neural network） を挙げられるでしょう。

   >>> coder も voder もは合わせて auto-encoder network のように振る舞います。そしてこれらは、人間による何らかのプロセスを含まずに、実際の音声データを用いて一緒に訓練されます。
   
   auto-encoder！私は arxiv でこれが一行でも出てくるような新規論文には必ず目を通すくらいには好きな深層学習の分野です。(~~実際この論文を真面目に読もうと思ったのもこの一行のためです~~)これは生成モデルと呼ばれる深層学習モデルの一つで、主には入力データと同じようなデータを作成することを目的にしています。例えば自動でイラストを作ってくれるモデルなんかはこの分野の研究分野の産物と言えるでしょう（正確にはこの分野で現在脚光を浴びているのはGANs(Generative Adversarial Nets) と呼ばれるモデルの分野です）。

   >>> Copy-Sysnthesis タスクに関する実験は、RawNetが LPCNet に比べてより小さいモデルながら高品質な音声品質を合成することが出来た他、推論時により高速に音声合成ができることが明らかになりました。
   
   つまり比較実験として LPCNet を用いたところ、小さいモデルで高品質、そして推論時により高速に音声が作れた、というわけです。

* RawNet は何が出来たの？
  一行で言うと、深層学習を用いたより高性能な vocoder を提案しました。
  
  また、

  1. そしていくつかの技術を用いることで voder と coder を同時に訓練する困難さを和らげモデルのパフォーマンスを向上させたことや、

  2. その結果人間からの評価としてこのモデルがより LPCNet に比べてより自然で高評価が得られる音声を生成できること、
     
  3. 音声特徴を視覚化して確認したところ良い感じの特徴が抽出できていることが確認できたこと


  も挙げられるでしょう。

  また、本論文ではこの研究を元にして他の音声合成フレームワークやその他の音声合成関連のタスクで応用されることを期待しているとのことです。この手の分野の研究がより進んでくれると [対話システムの研究でも](https://qiita.com/MeguruMokke/items/561e778ccd69e5160c74) 役立つのでぜひとも進めてほしいですね。

* RawNet の中身は？
  ここからは詳しい内容を見てみましょう。

  まずはじめにモデルの概要図を論文中から引用しましょう。このモデルは数式がほとんど出てきませんが、そういうものだと思ってください。
  
  #+ATTR_LATEX: :width 500%
  [[./img/rawnet_arch.PNG]]

  
  上部分が coder (auto-encoder でいう encoder) で、下部分が voder (auto-encoder でいう decoder) です。 waveform というのは音声波形になります。

  waveform から coder を流れていき、feature、特徴量を獲得し、そこから voder を流れることで元の waveform を得る、これがこのモデルの流れになります。
  
  順番に見ていきましょう。
   
** Coder と Voder
   - Coder    
     Coder は入力されるデータである音声波形から特徴量を抽出するための機構であり、その主な構成物は、多層型の畳込みレイヤと、Dense layer、そして GRU (RNN の一種、特に速度が早い) レイヤーです。多層型の畳込みレイヤによって高レベルの表現を学習することができると期待されており、言い換えれば一連の低レベルの（薄い）フィルタを何枚も通されることによって高レベルの表現の学習が期待されています。（このアイデアは [SoundNet](https://arxiv.org/abs/1610.09001) から得られたものだそうです。音声(動画？)分類問題の教師なし学習のタスクという面白い研究なのでちらっと見ると面白いかもしれません。）また後半の Dense layer と GRU が組み込まれている理由は音声波形の長期的な関係を学習するためのものだそうです。GRUが長期的な関係を学習するために役立つ頃は容易にわかりますが、Dense Layerは手続き上必要なもの以上の意味を感じ取れませんでした・・・
     またこの部分では可変長である音声波形の入力データを処理するために、畳込みレイヤーのストライド間隔、Pooling Layer の Pooling サイズを調節したそうです([参考資料](https://deepage.net/deep_learning/2016/11/07/convolutional_neural_network.html#%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89))。畳み込みレイヤはその位置に対して不変であるため（CNN の位置不変性と呼ばれることもあります）、出力長を制御するためにその各レイヤを畳み込むことができます。そのため、feature のフレームサイズは畳み込み層や Pooling Layer でのみ決定できることになります。
** Sampling Method

** Noise injection

** Post-synthesis denoising


* 実装はどこにあるの？
  [レポジトリ](https://github.com/candlewill/RawNet) が github にあるようですが、現在は中身がないようです。暇な人、実装してください orz

* 感想
  この論文自体はかなり面白いと思っているのですが、それ以上に様々な応用が考えられそうな内容であると私は思っています。
  
  例えば、上でも少し提案していましたが、feature の部分を応用してText-to-Speech が作れそう、という案が挙げられます。それは例えばテキストから直接 feature を作ることや、テキストの潜在表現と feature をつなぐとかです(恐らくこれは短文や句点区切りのデータのみにドメインを絞ることである程度の品質が見込めると思っています)。これがなぜできると想像しているのかと言うと、機械翻訳の分野である言語を VAE で学習し、それを２つ用意して、潜在表現間をDNNでつなぐという研究を読んだことがあるからです（この論文のタイトルを忘れてしまったので誰か知っていらっしゃれば教えてください）
  
  とはいえ、このあたりの研究は私では出来ない（十分な計算機、データ収集やアンケートの実施のための資金が降ってこない）ので、どこかの大学の方、研究所の方、研究の種にしていただけると幸いです。
* 音声合成ってなぁに？ :noexport:
  音声合成(Text-to-speech) を簡単に説明すると、何らかのテキストの記号（例えばツイートのようなテキストの文、或いは何らかのバイナリデータ）から音声を生成する研究を指します。
  音声合成は必ずしもテキストから音声を直ちに作ることを目標としているわけではなく、対話システムのようにいくつかの小さな分野を組み合わせて音声が作られる場合もあります。
  
  音声合成に関連する分野としては、深層学習を始めとした機械学習、自然言語処理などもありますし、また棒読みちゃんのようなシステムも含まれます。
  
  音声合成技術を用いている具体例としては、例えば Amazon Alexa や Cortana , Siri の音声生成を行っている部分（但しこちらは必ずしも機械学習を使っているわけではないです。機械学習は音声合成のための技術の一つです）を挙げることができるでしょう。

  日本語での面白い研究としては、[『ディープラーニングの力で結月ゆかりの声になるリポジトリ』の ipynb ブランチのリアルタイム変換サンプル](https://qiita.com/atticatticattic/items/f3fab0260a84ec121d4c) にある内容なんかは手軽に理解できるものとして挙げられるでしょう。

* 音声合成の関連研究はどんなものがあるの？ :noexport:
  深層学習を用いたものとしては、 [2017年～2018年4月までのディープラーニングを用いたText to speech手法まとめ](https://qiita.com/tosaka2/items/3564050fe0ccea360610) で概要が容易に確認できるでしょう。

  
  深層学習を用いないものとしては、「VOICEROID 仕組み」あたりで検索すると良いでしょう。例えば [初音ミクとかの音声合成のしくみ] (http://recognition.web.fc2.com/synthe/) なんかは簡単に触れることができる良い記事だと思います。
