#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: 高速に音声分析合成を行う RawNet について読む
#+date: <2019-03-17 Sun>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 音声分析合成ってなぁに？
  音声分析合成というのは、何らかの音声を合成して、それを分析し何らかのデータにして、それを複合して元の音声を生成する、という研究分野です。
  
  ある意味では音声合成（Text-to-speech）はこの一分野としてみなすことができるかもしれません。

  但し本研究はテキストから音声を合成することは出来ないと思われます。（多分テキストから潜在表現への変換をDNNで繋げればある程度可能だと思われますが、Google 音声合成API使えばよくね？と指摘されて [研究資源が降ってこないことが目に見えている](https://qiita.com/MeguruMokke/items/de3c0665ec2fb5ba2110) のでやりません）

* 音声分析合成の関連研究はどんなものがあるの？
  日本語でかなり有名なものとして、[WORLD](https://www.jstage.jst.go.jp/article/transinf/E99.D/7/E99.D_2015EDP7457/_pdf/-char/en)というものがあります。
  このあたり [1](http://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/) [2](https://qiita.com/misumi3104/items/de49e591a72b781c0521) が参考資料としてよいでしょう。
  ここにはないものとして、[LPCNet](https://arxiv.org/pdf/1810.11846.pdf) というものがあり、論文では触れられていますが、こちらについてはそのうち書きます。（~~[Mozilla](https://github.com/mozilla/LPCNet)あたりの研究で、それなりのネームバリューがあります~~）
  
* RawNet の概要は何？ 
  [RawNet](https://arxiv.org/pdf/1904.05351.pdf) は 深層学習を用いた 高速な Vocoder です。Vocoder とは何かと言われると日本語でうまいこと表現する語が思いつきませんが、音声合成するもの、と思っていただければ大丈夫でしょう。

  ちょっと論文の概要を翻訳してみましょう。
  
  >>> 昨今、深層学習を用いた vocoder は高品質の音声を生成することが可能であることがわかってきました。これらのモデルはおおよそメルスペクトラムといったスペクトル特性を用いてサンプルデータを生成します。しかし、それらの特性は人間の知識を元にしてできた（手打ちで作られた）プロセスを含んだ音声分析モジュールを用いて抽出されています。

  つまり、昨今の深層学習を用いた vocoder は学習データを作成する際に人手を使っておりコストがかかってしまっていることを指摘しています。

  スペクトル特性を用いてサンプルデータを生成する大きな利点としては、今の所こちらのほうが速度面で有利な結果が出ていることが挙げられるでしょう。なぜなら音声を直接生成しようとすると、何ミリ秒で１サンプルといった物凄い勢いでデータを推論する機構が必要になるためです。音声サンプリングそんなにたくさんなくても音声再生できるやろ？と思う方もいますが、これはシャノンの定理あたりを勉強するとなんとなくわかると思います。

  >>> 本研究では RawNet と呼ばれるモデルを提案します。これは真に end-to-end な vocoder で、信号のより高次元な表現を学習するために coder ネットワークを使用し、そのサンプルごとに音声サンプルを生成する autoregressive voder ネットワークを使用しています。

   vocoder とは coder と voder で構成されており、 coder は音声を符号化する機能を持っており、voder はそれを複合する機能を持っていると考えてください。このあたりは mp3 のような codec を想定してもらうとわかりやすいでしょう。mp3 では入力される音声をルールに従って圧縮したバイナリデータに変換し、これを同じルールに従って複合することで音声として我々が聞き取ります。
   
   autoregressive ネットワークとは、日本語では自己回帰ネットワークとも言われ、例えば RNN （recurent neural network） を挙げられるでしょう。

   >>> coder も voder もは合わせて auto-encoder network のように振る舞います。そしてこれらは、人間による何らかのプロセスを含まずに、実際の音声データを用いて一緒に訓練されます。
   
   auto-encoder！私は arxiv でこれが一行でも出てくるような新規論文には必ず目を通すくらいには好きな深層学習の分野です。(~~実際この論文を真面目に読もうと思ったのもこの一行のためです~~)これは生成モデルと呼ばれる深層学習モデルの一つで、主には入力データと同じようなデータを作成することを目的にしています。例えば自動でイラストを作ってくれるモデルなんかはこの分野の研究分野の産物と言えるでしょう（正確にはこの分野で現在脚光を浴びているのはGANs(Generative Adversarial Nets) と呼ばれるモデルの分野です）。

   >>> Copy-Sysnthesis タスクに関する実験は、RawNetが LPCNet に比べてより小さいモデルながら高品質な音声品質を合成することが出来た他、推論時により高速に音声合成ができることが明らかになりました。
   
   つまり比較実験として LPCNet を用いたところ、小さいモデルで高品質、そして推論時により高速に音声が作れた、というわけです。

* RawNet は何が出来たの？
  一行で言うと、深層学習を用いたより高性能な vocoder を提案しました。
  
  また、

  1. そしていくつかの技術を用いることで voder と coder を同時に訓練する困難さを和らげモデルのパフォーマンスを向上させたことや、

  2. その結果人間からの評価としてこのモデルがより LPCNet に比べてより自然で高評価が得られる音声を生成できること、
     
  3. 音声特徴を視覚化して確認したところ良い感じの特徴が抽出できていることが確認できたこと


  も挙げられるでしょう。
  
* RawNet の中身は？
  ここからは詳しい内容を見てみましょう。
* 実装はどこにあるの？
  [レポジトリ](https://github.com/candlewill/RawNet) が github にあるようですが、現在は中身がないようです。暇な人、実装してください orz


* 音声合成ってなぁに？ :noexport:
  音声合成(Text-to-speech) を簡単に説明すると、何らかのテキストの記号（例えばツイートのようなテキストの文、或いは何らかのバイナリデータ）から音声を生成する研究を指します。
  音声合成は必ずしもテキストから音声を直ちに作ることを目標としているわけではなく、対話システムのようにいくつかの小さな分野を組み合わせて音声が作られる場合もあります。
  
  音声合成に関連する分野としては、深層学習を始めとした機械学習、自然言語処理などもありますし、また棒読みちゃんのようなシステムも含まれます。
  
  音声合成技術を用いている具体例としては、例えば Amazon Alexa や Cortana , Siri の音声生成を行っている部分（但しこちらは必ずしも機械学習を使っているわけではないです。機械学習は音声合成のための技術の一つです）を挙げることができるでしょう。

  日本語での面白い研究としては、[『ディープラーニングの力で結月ゆかりの声になるリポジトリ』の ipynb ブランチのリアルタイム変換サンプル](https://qiita.com/atticatticattic/items/f3fab0260a84ec121d4c) にある内容なんかは手軽に理解できるものとして挙げられるでしょう。

* 音声合成の関連研究はどんなものがあるの？ :noexport:
  深層学習を用いたものとしては、 [2017年～2018年4月までのディープラーニングを用いたText to speech手法まとめ](https://qiita.com/tosaka2/items/3564050fe0ccea360610) で概要が容易に確認できるでしょう。

  
  深層学習を用いないものとしては、「VOICEROID 仕組み」あたりで検索すると良いでしょう。例えば [初音ミクとかの音声合成のしくみ] (http://recognition.web.fc2.com/synthe/) なんかは簡単に触れることができる良い記事だと思います。
