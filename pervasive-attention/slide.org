#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: 畳み込みな機械翻訳手法、Pervasive attention を読む
#+date: <2019-04-24 Wed>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 導入
  [[https://www.aclweb.org/anthology/K18-1010][Pervasive Attention]] というのは 2018 年 11 月に CoNLL (Conference on Computational Natural Language Learning) に掲載された機械翻訳に関する論文です。個人的なこいつの凄いところは Convolutional Neural Network を用いた機械翻訳手法で当時の機械翻訳のSOTAに食い込んだという点だと思っています。ある意味では、Transformer が Attention Is All You Need (後述するAttention機構に注視したモデルであるという意味)とされるならば、Pervasive Attention は CNN Is All You Need とも言えるようなモデルです。
  
  Transformerで沸き返っている界隈からすればやや日陰者感がありますが、それでも大変趣深い論文なので読んでみることにし、ついでに紹介ができればということで記事にしました。
* 機械翻訳って何？
  機械翻訳というのはある自然言語（英語とか）からある別な言語（ドイツ語）へ **翻訳** を行う、 **自然言語処理の中の一タスク** です。こいつができると **Google 翻訳** みたいなことができるようになります。

* 機械翻訳の関連研究って何？CNN で機械翻訳？
  かつては辞書を用いた機械翻訳などがありましたが、最近では深層学習を用いたものが有力になっています。というわけで今回は深層学習の、特に論文中に取り上げられているものを、以下の３つの観点から眺めてみます。

** 一般的な(昔からある)機械翻訳の手法 (RNN を用いた encoder-decoder モデル)
   恐らくこの分野の研究で一番ベーシックなものとして考えて良いのがこの形のモデルでしょう。これは RNN(Reccurent Neural Network) を用いた、AutoEncoderのようなモデルです。以下の概略図を見てください。EncodeとDecoder というブロックが latent vector というベクトルで繋げられていることがわかると思います。Encoder は入力文を単位ごと（例えば単語ごと）に RNN セルに通すことを次々に繰り返します。Decoder は先頭を示す記号(<s>)を頭につけた出力文を単位ごとに RNN セルに通すことを次々に繰り返します。そして Decoder の RNN セルからの出力が期待する翻訳文となります。機械翻訳に対して最も影響力のあったモデルはこの形の、Googleの研究者らが発表した Sequence to Sequence[2] というモデルであり、こいつは Seq2Seq という略称で親しまれています。

   #+ATTR_LATEX: 500%
   [[./img/seq2seq_arch.png]]


   これに対して改良策として RNN セルを LSTM/GLU/BiRNN/BiLSTM セルに置き換えたり、Attention機構 (Encoder の途中の情報を Decoder に直接的に反映させていく考え方、詳細は [Luong et al. 2015][1]) を加える考え方などが提案されています。

[1]: Effective approaches to attention-based neural machine translation. (Luong, H. Pham, and C. Manning. 2015.)
[2]: Sequence to Sequence Learning with Neural Networks (Ilya Sutskever, Oriol Vinyals, Quoc V. Le)

** CNN を用いた手法
   脚光を浴びていたRNNを用いた手法ですが、訓練時間的な問題があったりするため、CNNで代替できないかという話が持ち上がっています。

   もう少し理論的な話をすると、RNNやLSTMは時間的に並行に学習することが構造上難しいのに対して、CNNは並列に計算することが可能な点がこの考えのモチベーションの一つになっています。またAttention機構がCNNの構造的に最初から含まれているという点も挙げられます。(まとめて畳み込んで出力しているので、実質Attention、みたいなことらしいです。尚、他のCNNを用いた関連手法すべてにこれが適応されるかは…謎です)

   関連手法として有名なものを挙げるとすると、Convolutional Sequence to Sequence Learning[3] が挙げられます。これは Facebook の研究者が出したペーパーです。詳しい説明については複雑なので省略しますが、この [[http://deeplearning.hatenablog.com/entry/convs2s][論文解説]] はとてもわかり易く解説しているので興味があれば是非ご一読ください。
   
   [3]: Convolutional Sequence to Sequence Learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin

** その他(Attention is all you need)
   その他の例としては上の２つと大きく異なる構造を持った Transformer というモデルが有名です。Attention is all you need[4] という論文で提案されたモデルで、昨今の自然言語処理のスイスナイフばりの立ち位置を確立しています。(どこかのツイッタラーさんが、自然言語処理を始めるならMeCabなんぞ捨ててまずBERT[5] をやってみろ！みたいなことをツイートしていましたが、このBERTのベースとなる手法です。)
   
   この特徴としては、一般的な(昔からある)機械翻訳の手法 でちらっと触れた Attention機構を注視してRNNの機構を取り去ったようなモデル、ということになっています(少なくとも界隈ではそう言われています)。この解説としては [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer]] のページを見ると良いです。(英語の記事に対して中国語、韓国語の翻訳はあるのに日本語の翻訳がないところが、この国の技術力って感じで趣がありますね)
   
   
   [4]: Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
   [5]: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

* どのくらいの性能が出来たの？
  #+CAPTION: Pervasive Attention :  Table 3
  #+ATTR_LATEX: :width 500%
  [[./img/pa_results.PNG]]

  大体のスコアはこんな感じになっています。De-En/En-Deというのは、BLEU評価で、簡単に言うと0-100の連続値を取る評価で高いほうが良いです。これを見る限り、Pervasive Attention は割とつよつよ、という感じになっています。また計算量 Flops を見ると、Transformerよりも少ないものの、全体的に見ると少し多いかな？という印象があります。しかしパラメータ数 prms を見ると、相当に抑えられていることがわかります。

  つまり簡単にいうと、**そこそこの計算量・小さいパラメータ・つよつよのBLEUスコア** ということになります。

  尚、Word-basedというのは文を単位に区切る際に単語ごとに区切るということ、BPE(byte-pair encoding)-based というのは（簡単に言うと）単語分割よりも語彙数を抑えられるように文を単位に分割していくことです。BPEの最近の発展としては、日本でやたら滅多に有名で有力な万能ツールと叫ばれている SentencePiece なんかがありますね。SentencePieceはあんまり詳しくないですが [[https://qiita.com/taku910/items/7e52f1e58d0ea6e7859c][Sentencepiece : ニューラル言語処理向けトークナイザ]] が最も信頼における記事だと思います。

  しかしこの実験結果には少しだけ問題があります。それは用いたデータセットである IWSLT はそんなに大規模なモデル (参考までに紹介すると IWSLT は 200K サンプル /　WMT は 15.8M サンプル) とは言えないという点、多言語での実験結果がないため、別言語でどうなるかはなんとも言えないという点です。(日本語のこういったコーパスはやたら高い上に制約も強いので Fr-En あたりで実験してみたいものです)
  
  また別の着眼点としては、入力文の長さに対してどのようなBLEUスコアがつくか、というものがあります。つまり短い文と長い文をこのモデルに突っ込んだらどのくらい精度の差がでるの？ということです。
  
  下の図を見てください。これを見ると、特に短い文に対して高い性能が出ていることがわかります。逆に長い文に対しては比較的に強くはないようです。
  
  #+CAPTION: Pervasive Attention :  Figure 5
  #+ATTR_LATEX: :width 500%
  [[./img/pa_bleu_len.PNG]]

* この構造はどんな形をしているの？
  さてそれでは簡単にモデルの概要図を説明していきましょう。

  まず入力文と出力文を畳み込みできる形にしてみましょう。以下の図を見てください。大体こんな感じで入力文と出力文を畳み込み可能な形(つまりTensor)に構成しています。なんとなく入力と出力がクロスしていること、コピーをして行列サイズを合わせていることが想像できれば良いと思います。
  
  #+CAPTION: Pervasive Attention :  Figure 5
  #+ATTR_LATEX: :width 500%
  [[./img/input_architecture.png]]


  
  
* 結論とか読んだ感想とか
  
