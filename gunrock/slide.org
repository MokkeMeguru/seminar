#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Amazon Alexa の内部システムの最先端を読む（2018）
#+date: <2019-03-17 Sun>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)

* 導入
  この記事は Amazon Alexa Prize 2018 年度最優秀賞を獲得した Gunrock の論文を元に作成しています。    

  他の論文では少し内容が異なっていることがありますが、最優秀賞を取ることができたということは恐らく最も出題を理解していたのだろうということでこちらを参考にしました。    

  また本記事の方針は、対話システムってどんな研究しているの？ということを紹介することです。なのでこの記事を読んでも *Alexa は作れません* (とはいえ本家の論文を読んでも再現性ないですが)。    
* Amazon Alexa Prize 2018 とその最優秀賞である Gunrock とは
  昨年の冬、Amazon Alexa の内部システムを競うコンテスト、Amazon Alexa Prize 2018 の審査結果が発表されました。最優秀賞を獲得したのは Gunrock というシステムで University of California Davis によるものでした。    

  今回（昨年度もほとんど同じような内容だったような・・・）のテーマは Social bot と呼ばれる主に雑談対話をメインに据えた対話システムの作成でした。とはいえ我々の想像以上にこのコンテストはやらなければ多く、例えば音声認識や音声生成、Alexa特有の処理へのハンドリングを挙げることができます。    

  さて件の Gunrock は本テーマにおいて最も良い評価を得ることができたシステムとなります。評価基準は審査員からの5点満点の評価で、3.62 点を取りました。これが良いか悪いかは皆さんの主観に任せますが、まだまだ改善の余地があると個人的には感じています（というのも Davis チーム曰く、このコンテストは期間が結構きつく、詰めきれない部分があったようです）。そして本システムの特徴として挙げられるものとして、審査員の平均対話時間が 5分 22秒を達成したことがあります。つまり「Hey, Siri！」と言ってから「Bye, Siri!」 となるまでということです。5分間って短くね？と思う方もいるかもしれませんが、 *言葉だけ* でこの時間喋るのは結構凄いことだと思います。（とはいえ昨年度優勝した Sounding Board はこれよりも長い対話時間を達成していました）    

  Gunrock が特に取り組んだ問題は、以下の3つとなります。    

  1. オープンドメインの音声言語理解を処理するための3層音声言語理解パイプラインの提案
  2. 対話の文脈情報を利用して 一般の事実 と botの意見 をシームレスに切り替えていく柔軟な対話フローを可能にする階層的な対話管理システムの設計
  3. トーン調整や休符挿入を行うことでより自然な応答発話を構築する韻律音声合成機構の作成
  なんのこっちゃと思う方もいると思いますが、私もなんのこっちゃという顔をしています。というのもまるで違う研究成果が一つの論文にまとめられている（しかも19ページしかない！）ため、一章読むたびに全く違う分野の参考文献を探す羽目になるという地獄を味わいます。間違っても個人でこの分野は研究したくないなと思いました。まあ、やるんですけど

  ・・・とりあえずこのコンテストについてもう少しだけ触れてみましょう。
* Amazon Alexa Prize 2018 とは何だったのか
  Amazon Alexa Prize は Amazon Alexa の内部システムを作るコンテストです。このコンテストには毎年テーマが設けられ、今年のテーマは 「Social bot」なるものの作成でした。Social bot とは所謂雑談対話のできる bot (chatbot などと呼ばれていますが、日本の蔑称的な意味合いはないです）と Microsoft Cortana のようなユーザの動作補助をする Personal Assistant の中間にあるような対話システムです。具体的には以下の図 (sounding board のスライド(https://sounding-board.github.io/index_files/Ostendorf_naacl2018.pdf)より)を参照してください。

  いまいち Social Conversation の意味に掴みかねる部分があるかもしれませんが、簡単にいうと世間話です。またこの世間話というのがかなり難しい問題で、カタカナで格好良く言うならば、オープンドメインな対話、ということになり、つまりほとんど至るところの話題に対応できる雑談対話システムということになります。
  
  #+ATTR_LATEX: :width 500%
  [[./img/soundingboard.PNG]]
  
  つまり世間話ができて、ユーザの望む何らかのタスクも解ける、そんなタスク指向型の対話システムと非タスク指向の対話システムを組み合わせたような対話システムを作ってくれ、ということになります。

  ここで注意しなければならない点として、これは *英語圏の人向けの英語の対話システムのコンテスト* だったということです。世の中には、日本も海外もコミュニケーション方法は大差ないし、英語から日本語にちゃちゃっと変えるだけで直ちに日本語対話システムを作れるだろうと言う人もいたりするのですが、論文や関連研究を読む・調べる限りそう容易なものではないので、日本語のそれを作りたい方はお互い頑張りましょう・・・
  
  大会期間は2018年の春終わりから夏くらいまでだったようですが、恐らく事前準備していた団体の方が多いのであんまり参考にはしないほうがいいかなと思います。

* Social bot の関連研究に触れる
  関連研究としてはタスク指向の対話システム、オープンドメインな対話システムそれぞれで見れば広く研究されているようです。
  
  前者は例えば Line なんかで出前の受付とか、ちょっとテクノロジアな企業のWebページにあるヘルプデスクなんかがこれになります。
  
  後者は、時代によって解釈がやや異なるようです。例えば時代を20年位遡ってオープンドメインな対話システムについて議論すると、それは Turing Test と呼ばれる、人間と遜色ない対話を可能にする対話システムを指し、研究としては、Alice といった chatbot が該当します。逆に現代でそれについて議論すると、Amazon Alexa や Google Assistant、Siri のような短い対話を行えるような、或いは質疑応答ができる対話システムを指します。
  
  また深層学習を用いたモデル (Seq2Seq や Transformer, HRED, VHREDなんかです。ちなみに実験したところ Transformer はこの分野でも結構良い結果（BLEU評価）が出ました)や強化学習を用いたモデルも先行研究として挙げることができますが、これらは対話の一貫性が得づらいという問題や、会話の多様性が損なわれるという問題もあるようで満足な結果を得るのはまだ難しいようです。
  
  そのため、昨年度 Social bot として優秀な成績を残した Sounding Board はルールベースのモデルと end-to-end なモデルを組み合わせるアプローチを活用していました。これはユーザエクスペリエンスを向上させることができ、対話時間を伸ばすことができるとわかっています（実際Gunrockよりも長い対話時間を達成していました）。しかしこの手法にはいくつかの欠点があると Gunrock のチームは指摘しています。それは新規性のある話題について対応することが難しいこと、ユーザからの意見に基づく要求をうまく処理することができないということです。（end-to-end モデルなので、そういった要求に弱い、ということのようです。）
  
  #+CAPTION: https://www.apple.com/jp/siri/
  #+ATTR_LATEX: :width 500%
  [[./img/siri.PNG]]

* Gunrock のアーキテクチャを見てみる
  :PROPERTIES:
  :ORDERED:  t
  :END:
  さて、Gunrock が一体何をしようとしたのかわかってきたところで、どういう仕組みなのかを見ていきましょう。端的に言うと、以下の論文中の画像一枚で済ます。
 #+CAPTION: https://s3.amazonaws.com/dex-microsites-prod/alexaprize/2018/papers/Gunrock.pdf
  #+ATTR_LATEX: :width 800% 
  [[./img/gunrock_ovreview.PNG]]

 とはいえ初見でこれは理解が難しいと思うので（理解できれば後の文章は読み飛ばしてください）ちょいちょいと説明を加えていきます。

 まず左上の *User* に注目してください。これは人間を指しています。つまり今椅子に座っているなりして画面を見ているあなたです。発言は矢印を進んでASRへ進んでいきます。この時点でいう発言は、*音声* となっています。イメージとしてはギジャギジャした例の音声波形みたいな感じです。

 ASR (automatic speech recognition) では音声を文字に書き起こす機能を提供します。Gunrockではこの部分は Amazon が提供している ASR の機能を用いたほか、それのエラーに対処する機構を作りました。それは例えば、あんまりにも信用ならないようなテキストがASRから得られた場合に、それを切り捨ててユーザにもう一度繰り返してもらうないし別の表現で喋ってもらう機能です。まあ結局、なんやかんやあってASRから``hello alexa my name is siri" みたいなテキストが出てきます。

 そのテキストは次に Neural Language Understanding（一般にはNLUと言われているようです）というでっかいシステムへポイされます。ここでは上から下へ入力のテキストが *流れていきます* 。この流れる処理設計＋スレッドプール設計が処理速度面で効いたと論文中では言っています。

 この中身については後で簡単に触れておきましょう。今はなんやかんやしてうまいこと入力されたテキストに対して、カテゴリなどの情報抽出が出来たということにしてください。
 
 すると次にそれらが行くのは Dialog Manager という部分になります。ここはGunrock独自のシステムで、左側と右側で上下関係が出来ています。

 上位にあたる Intent Classifier はユーザの3段階のプロセスで対話意図を分類します。１段階目は例えば "Play music" や "set the temparature" といったAlexaへの命令のようなものです。これに関してはAlexaの機能（音楽をかけるとか）を起動するように促します。２段階目は話題のカテゴリを特定することで、NLUから得られる付加情報（一部後述しますが、Google Knowledge GraphやMicrosoft Concept Graph、Amazon Alexa Prize Topic Classifier なんかから得られるカテゴリ情報です）を元に分類を行います。３段階目は lexical intents と名付けられたもので、ユーザが Gunrock に向かって好みや意見について質問しているのかどうかなど、ユーザの要求を分析するためのものです。これには正規表現を用いたようです。
 
 上位で処理されたものは、更に Dialog Module Selector (図には書かれていないですが、Intent Classifier -> Topic Dialog Modules の部分にあると思ってください。)で Topic dialog Module の分類が行われます。ここで行われていることは、Intent Classifierで行われた情報と、それまでの対話履歴を元に、どの Topic Dialog Module でそのユーザからの発話が処理されるべきかを最終的に決定することで、たとえば対話履歴からその話題について話されることがないとわかっているならば（例えば``話題を変えましょう”と事前に言われていたときなど）はもしその話題に行くべきだと前の部分が言っていたとしてもそれを受け付けず、別の Topic Dialog Module に行くなどします。
 
 下位で処理されるものは、
 
* Gunrock の内部システムを深く見てみる (1)
  
 まず Segmentation です。ここでは入力をうまい感じに意味的に分割します。ちょっと難しいので例を出しましょう。例えば入力に、``alexa that is cool what do you think of the avengers''(かっこいいアレクサ、アヴェンジャーズについてどう思ってる？) という入力文があったとします。すると分割記号 <BRK> を用いて、``alexa <BRK> that is cool <BRK> what do you think of the avengers'' という風になります。これを行うために Gunrock は深層学習モデル (Seq2Seq モデル) と入力の音声データにおける空白時間(alexa || that is cool || what do you think of the avengers の ``||" の空白時間に注目したようです)を用いた検出機構を組み合わせました。

 次に Noun Phrase Extraction です。こちらは入力文から名詞句を抽出することを目的としています。名詞句を取り出すと、後述される入力文をカテゴリ分類するときや表現抽出(entity recognition)を行うときに、必要になる名詞句が手に入ることになります。将来的にはこれを発展させて目的語の意味を持っているのか、主語の意味を持っているのか、などの詳しい情報を抽出しこれ以降の手順（例えばカテゴリ分類などへ）活かしていきたいらしいです。
 
 抽出された名詞句は、Google Knowledge Graph や Microsoft Concept Graph 、そして文脈についての情報をを組み合わせてカテゴリ分類されます。Stanford CoreNLPや spaCy なんかのツールは、大文字・小文字にその機能を依存させているフシがあるらしく、この場合あまり効果が見込めないと判断したためらしいです。文脈についての情報ってなんだと思いますが、これはGunrockが内部で保持している、「我々は今何の話題を話しているのか」という情報を用いるようです。（つまり映画について話しているならば avengers は映画の名前かもしれない、と考えることができる、ということのようです。）
 
 また名詞句を抽出する際に話している文脈の情報に応じて、ASRのエラーを推測する機構も作りました。こいつが何をしているかと言うと、ASRで出てしまう同音語なんかをうまく処理するために、文脈情報を使っています。わかりやすく日本語の例を出しましょう。例えば食事の話題を話しているときに、「はし」という言葉がASRから得られていたとします。すると食事の話題でありますから、「橋」よりも「箸」のほうがそれっぽいですよね。というわけGunrock君は「はし」ｰ>「箸」のほうがそれっぽいだろうと推測します。このようにして文脈情報を使ってASRから得られるテキストを修正するわけです。またこれを行うために double metaphone algorithm を用いていましたが、これはちょっとあまりにも英語英語しているので省略します（英語の発音は詳しくないのです・・・）。
 
 更に Coreference Resolution (日本語で言うなら、相互参照の解消となりますがちょっと想像しにくいですね)を行っています。これは例えば文中に現れる ``it, one" などが意味的に（つまりそれが何を指し示しているか）置換します。ちなみに既存手法（Stanford CoreNLPやNeuralCorefのSOTAなもの）は非会話データを元にして訓練されていたので、会話データにはあまり適していなかったそうです。そこでGunrockでは相互参照先になるであろういくつかのキーワード（oneなど）をラベル付けし、それをユーザの発話やシステムの情報なんかを元に置換したそうです。（こういう対象となる問題が異なるために同じタスクでもモデルの適性が統一されないのは、とても難しいところですね。SOTAだからって何でもかんでも解けると思わない方が良いということでしょうか）

 Dialog Act Prediction（対話上の役割予測）とは
 

* Gunrock の内部システムを深く見てみる(2)
  
* Gunrock の内部システムを深く見てみる(3)

* Gunrock が残した課題

