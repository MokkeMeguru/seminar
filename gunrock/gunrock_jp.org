#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: gunrock_jp
#+date: <2019-03-17 Sun>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 概要
Gunrok は オープンドメインな対話システムで、ユーザに寄り添う形にデザインされた social bot です。
我々はより人間らしい bot を目指すため、大規模なユーザの対話データを繰り返しこの bot に与えました。例えば、このシステムは 2018 年度に行われた Amazon Alexa Prize の準決勝期間中に 4万回を超える会話経験を積みました。
内部については、話題の転換や質疑応答といったユーザの様々な振る舞いに対して、文脈に応じて階層的な会話を行うことのできる対話管理システムを適用しました。これに加え、我々は頑強な 3 ステップの自然言語理解のためのモジュール(NLU moduleRTX3900)を設計しました。この中の例としては、文の細分化や自動音声認識エラー修正システムを挙げることができます。更に、我々は bot が人間らしい振る舞いをすることができるように、韻律音声合成を行いました。
これらの多くの努力の結果として、この bot は 10月14日に行われたユーザによる 1-5 点評価のアンケート調査によって 3.62 点の評価を得ることができました。加えてこの調査では、平均 22.14 ターン、時間にして 5分 22秒の連続会話を達成することができました。
* 導入
対話システムの研究における一つの重要な課題として、より多くのユーザによってそのシステムが訓練・テストされなければならないことを挙げることができる。これに対処するために、ほとんどの研究者は、クラウドソーシングされたプラットフォーム上で、コストをかけることで彼らのシステムを訓練・評価していました。しかし、そうやって訓練・評価されたシステムは実際の製品に適用した際に不安定な結果を招く可能性がありました。我々は45日の評価期間で、一日に500対話以上のデータを入手し、最終的に開発期間全体(~ 8月 14日)では 487,314 対話を入手することができました。例えば Amazon Echo といった Alexa が搭載されたどのようなマシンにおいても、アメリカ国内ならば我々のシステムを試すことができました。我々のシステムは多様なユーザの大規模なアクセスに対処する必要がありました。

人間は人間同士のコミュニケーションパターンに慣れているため、ほとんどのユーザは人間対人間のコミュニケーションが行えることをシステムに求めると我々は考えています。例えば、 Microsoft Cortana(パーソナルアシスタントの一種) はソーシャルコンテンツに対処することができないということが知られていますが、ユーザの発言の30％はソーシャルコンテンツを求めています。そのため、対話システムの性能を向上させる有効打の一つとして、人間のコミュニケーションを模倣させることが考えられます。そこで我々は特定の人気のある話題について深く会話することができるという能力を持ち、多様な社会的トピックを網羅することができる、人間対人間の自然な対話を模倣する オープンドメインな social bot である Gunrock を提案しました。これによって我々は、オープンドメインな発話理解・対話管理・そして自然言語生成の分野に多くの貢献をすることができました。

このオープンドメインな発話理解には2つの大きな課題がありました。それは、1. ASR(自動音声認識) エラーと 2. 実体(エンティティ)の曖昧さです。我々はこの問題に対して、新たに3段階の自然言語理解(NLU)のパイプラインを設計しました。というのも、ユーザは一度に複数の文を繋げて話すことができるものの、ASRはそれを一つの文として認識してしまい複数の文に分割することができません。我々のNLUは、これに対処するために、まずASRから入ってくる原文を複数の小さなセグメントに分割します。そしてそれらの小さなセグメントに様々なNLPの技術を用いて固有表現(named entity)・対話の意図・感情についての情報を抽出します。最後に、相互参照・ASRエラー・エンティティの曖昧さの解決のために、文脈情報や音声情報を活用します。

更に我々は、ユーザ間の様々な異なる会話に対処するための、階層的な積立形式の（stack-based) 対話管理システムを設計しました。この対話管理システムは第一に、NLUから得られる情報を用いてユーザのリクエストを映画やスポーツといったトピックに振り分ける高レベルの決定機関を持ちます。そして振り分けられたトピックに対応する固有のトピック対話モジュールが呼び出されます。各トピック対話モジュールでは、より詳細で包括的な会話にユーザを参加させるために役立つ定義済みの会話フローが組み込まれています。様々なユーザの行動に対応し、会話の一貫性を保つために、システムはこのフローに出入りして、いつでも事実に関する/個人的な質問に答えることができます。これに加えて異なるドメイン固有のトピック対話モジュール間に作成されたトンネルを使用して、ユーザの意図的な話題の切り替えに対応できるようにします。

システムの発話生成については、そｓの内容と同じ位に重要な要素です。より人間的な相互作用を形成するために、Amazonの音声合成マークアップ言語 (SSML) を用いて韻律効果のライブラリを作成しました。我々は、ユーザからのインタビューから、韻律効果や間投詞がより自然であるように彼らから求められていることがわかりました。
* 関連研究
タスク指向な対話システムや、オープンドメインな対話システムは広く研究されています。前者は特にレストランの予約といった場面に研究されています。オープンドメインなものでは、Alice のような古代のチャットボットが Turing Test をクリアするために作成されていたのに対して、Amazon Alexa や Google Assistant のような最近の対話システムはユーザとの短いターンの対話や、質疑応答の対話に対して焦点を当てています。これらに対して、social chatbot は感情的なサポートを含めた深いコミュニケーションスキルが要求されます。Gunrock は タスク指向・オープンドメインの両方の分野の SOTA (state-of-the-art) の技術を利用し、より柔軟にユーザとの会話ができるようになっています。

多くの神経モデル（naural models）や強化学習モデルが、言語理解や文生成のために提案されています。 Cornell Movie Dialogs や Reddit といった大規模データセットに支えられ、それらの end-to-end なアプローチのモデルは対話の性能を向上させています。しかしこれらの手法は対話の一貫性がないことや多様性が得られにくいというような問題を抱えています。

この問題の解決策として、ルールベースと end-to-end のモデルを組み合わせるアプローチを用いたいくつかの研究が存在しています。他の関連研究としては、個々の小さな技術と知識グラフを活用しています。2017年度の Amazon Alexa Prize の優勝システムである Sounding Board はこの関連研究を活かしていると言えます。このアプローチはユーザエクスペリエンスを向上させ、対話時間を伸ばすことができることがわかったが、その一方で、この手法には新しい話題に対して柔軟な対応をすることができず、ユーザからの意見に基づく要求をしっかりと処理することができません。（知識グラフから対話を生成するという都合上、要求や意見に対して弱い、という意味のようです）

我々のシステムは異なるドメインのデータセットを連結させる利点を最大限に活かし、トピック対話モジュール間をシームレスに移行するトンネルを作成しました。我々は自然言語理解(natural language understanding NLU) と自然言語生成 (natural language generation NLG) のためにユーザから収集したデータに加え、先述のデータセットを利用してモデルを訓練しました。この新しいコンセプトは Amazon Alexa Priza で優秀な成績を残すことに非常に貢献したと言えるでしょう。
* システムの構成
我々は Amazon Conversational Bot Toolkit を活用してシステムアーキテクチャを構築しました。この Toolkit は、開発者がユーザフレンドリーな bot を構築することに集中できるよう、簡単にスケールできるフレームワークを提供します。そのイベント駆動ベースなシステムは AWS Lambda function 上で実装され、ユーザが bot にリクエストを送信されたときに呼び出されます。この bot のインフラとして、ユーザデータと対話の状態データの両方を DynamoDB に格納するための 状態管理インターフェースも挙げることができます。

我々は更に、Redis や Amazon が新たに提供したグラフデータベースである Neptune を内部の知識グラフを構築するために活用しています。
** システムの概要
   Figure 1 はこの social bot のシステムのフレームワークを示しています。Amazon は ASR モデルを通してユーザの発話をテキストとする部分と、Amazon Alexa の Text-To-Speech (TTS) を用いてテキストを発話に変換する部分を提供しています。我々のシステムの重点はテキストの入出力にあります。
待ち時間が長くなる可能性があるため、ユーザからの信頼性が低い・劣悪な・不完全な入力といった一部のシナリオについては、いくつかのモジュールを使用して応答文を生成します。例えば、もし ASR の結果が信頼におけるものではないと評価されたならば、ユーザに同じことを繰り返してもらうか、またはより明確に説明してもらえるよう求めるプロンプトを生成します。

ASRを通過した後、ユーザの入力は複数の NLU のコンポーネントに通されます。例えばそのコンポーネントは、 Amazon Toolkit のサービスや、対話行為の検出器(dialog act detector)といったものです。この内容については後に紹介します。

対面及びオンラインのいずれの場合にしても、人々は乱暴なな文脈を含む対話を行うことがあります。我々は Amazon Offensive Speech Classifier Toolkit を用いてそのような内容を排除できるようにしています。（おそらくTwitter上で活動していた対話AIが悲惨な（不適切な教育を受けてしまった）ことを考慮した対処でしょう）また、もし現在話している内容が不適切な兆候を示している場合には、その話題が不適切であることをユーザに知らせるとともに、別の話題を提供します。

Figure 1 では 12 のコンポーネントが NLU に含まれていることがわかります。待ち時間と依存関係の間のトレードオフとのバランスを取るために、このボットに組み込まれている自然言語処理のパイプラインとスレッドプール設計を利用します。NLU のパイプラインは3つのステップで構成されています。第一のステップでは、入力は複数の文に分割されます。次のステップでは、名詞句が検出されます。最後のステップでは、抽出された名詞句を元に Figure 1 の NLU の下部に示されるような詳細なコンポーネントで分類します。

対話管理システムでは、意図分類器（Intent Classifier）が異なるユーザの意図を対応するトピック対話モジュールに向けるために使用されます。それらは映画やスポーツ、動物といったいくつかの特定の話題をカバーしています。それぞれの対話モジュールは独自の対話フローを持っており、ユーザはその流れに沿って柔軟に深く会話を行うことができます。すべての対話モジュールは、Amazon の EVI サービス や back-story を利用して、例えば「あなたの好きな色は何ですか？」といった bot のペルソナに関連した質問に答えたり、現実の質問（例えば「今の米国大統領はだれ？」）に答えることができます。加えて、Amazon EC2 インスタンスを用いることで、複数のデータソースや、蓄積されている知識ベースからデータを収集します。文脈情報とともにNLUから得られたすべての情報は、応答を生成する役割を担う NLG の適切なモジュールを選択するために用いられます。

NLGシステムは、システムの応答テンプレートを一元管理するためにテンプレートマネージャ（Template Manager）を利用します。回答の健全性を確実にするために、回答内容を精査するための機構（Profanity Check）も含まれます。元のトピック対話モジュールの状態を更新するためのポストプロセッサ（Post Processor）もNLGに含まれています。最後に、Amazon SSMLを用いて生成文の韻律を調整します（Prosody）。
** Automatic Speech Recognition
   ユーザの入力はNLUに入る前に、ASRのエラーを処理するため、ASRから得られる全体的な信頼スコアとそれぞれの単語についての信頼スコアを元に前処理されます。本システムでは、信頼スコアに応じて以下の3種類のASRエラー応答を定義しました。
   1. Critical Range
      全体的な信頼スコアやそれぞれの単語についての信頼スコアが0.1を下回った場合、本システムは全体のパイプラインを一旦停止させ、ユーザに入力を繰り返すか別の言葉で表現するかを促します。
   2. Warning range
      全体的な信頼スコアやそれぞれの単語についての信頼スコアが0.4未満0.1以上であった場合、本システムはASRの修正を行うシステムへ入力を送ります。
   3. Safe range
      上のいずれでもなければ、入力をそのままASRによって得られた結果としてNLUへ送ります。

   　また、ユーザの予想外な入力（例えば苦情や不完全な文）に対しても処理を定義しています。一般にこのような状況では、単に情報を提供するだけでは、ユーザエクスペリエンスが低下する可能性があることが知られています。そのため、ASRでの前処理として、それらの異常な入力を検出し、それらを明確化するためのユーザへの応答を行う機能を作成しました。
** NLU (自然言語理解)
   Alexa Skills Kit(ASK) はNLUのためのトピック分類や、感情分類、不適切文検知、そしてNERの機能を提供しています。我々はこれに、ユーザからの入力を分割するための文分割モデル(sentence segmemtation model) を加えました。これによってユーザからの入力を意味単位に分け、各意味単位に対して他のNLUの機能を適用していきます。
*** 文分割(Sentence Segmentation)
    本システムは対話をより面白くするために、対話フローに束縛されないオープンドメインの質疑応答ができるよう構成されています。この質疑応答によってユーザがより対話を行うようになることが見込めますが、代償としてユーザの入力文がより複雑になる可能性があります。我々は文分割を行うための Seq2Seq model 型の(end-to-endな)モデルを Cornel Movie-Quotes Corpus で訓練しました。この Corpus には 304,713 対話と、ラベル付された検証データが 23,760 発話が含まれています。このデータは文の区切りに特別なトークンを付与するという前処理が行われています。このモデルは単語埋め込みとして Common Crawl のデータを元にした（このデータには200万単語が含まれています）300次元の fastText を用いています。Seq2Seq モデルとしては、2層の双方向LSTMをエンコーダ、2層のRNNをデコーダとして採用しており、global attention機構を用いています(恐らくこれは Luong Attention モデルの改変です)。単語埋め込みから同様の単語が生成されたとき、その特別なトークンを除いてその出力単語を入力単語と同じ単語になるように修正します（同じ単語を用いる方が文が繋がりやすいという予測からか？）。
    このモデルは30エポック訓練され、結果として検証データ 220 発話に対して 95.95% の精度(accuracy)を達成しました。これは事前訓練された言語モデル(?)よりも優れた精度を示しています。ここで文分割の例を示すと、``Alexa that is cool what do you think of the Avengers'' は ``Alexa <BRK> that is cool <BRK> what do you think of the Avengers <BRK>'' となります。
    さらにこのような文中の切れ目を効率的に検出するために、ユーザデータにラベル付をすることやASRから得られた結果を用いてユーザの発話記録に注釈をつけ各単語間の相対的な空白時間を利用しました。具体的には以下の確率を最大化しました。
    \begin{eqnarray}
    p(x_i|x_1, \dots , x_{i-1}, \cfrac{t_i}{\bar{t}})
    \end{eqnarray}
    ここで、$x_i$ とは入力された単語あるいは分割のシグナル (<BRK>) であり、$t_i$ は ASR から得られた $x_i$ と $x_{i-1}$ との間に起こる時間差、$\bar{t}$ はその平均です。
    この手法の課題点に、固有表現抽出と不完全な入力文への対応（恐らく日本語にある省略など？）があります。今後これらの問題については取り組んでいく予定です。
*** 名詞句の抽出(Noun Phrase Extraction)
    Stanford CoreNLP のパーサ (Stanford CoreNLP constituency parser) を用いることで、入力文を名詞句やローカルな名詞句(解析木の葉のレベルなもの)を抽出します。いくつかのストップワードに関してはフィルタリングを行い、残ったものを他のNLUのモジュールなどのキーワードとみなしました。
    将来的には、複数の名詞句の従属関係を解析し主語・目的語などを識別できるようにします。
*** 表現抽出（Entity Recognition）
    Stanford CoreNLP や spaCy といった NER を処理できるツールは、大文字・小文字といった単語の性質に強く依存していると考えており、音声認識から文字を書き起こす我々のシステムに対しては適用が難しいと考えています。加えてそれらのツールは、教師データに与えられていないような表現を抽出することが難しいです。このため、より多くのデータを用いて表現抽出をできるように、並行して以下の3つの表現抽出のためのプログラムを用いました。
    1. Google Knowledge Graph
       Google Knowledge Graph を利用して、名詞句について検索を行い、それに対するラベル・信頼度を生成し、その結果を Redis にキャッシュします。また、説明を我々の作成したモジュールにマッピングします。例えば、名詞句「tomb raider」は 「video game series」 というラベルが高い信頼度で付属しているので、これを game モジュールに対してマッピングします。さらに、名詞句を明確にするために複数のラベルを抽出します。（例えば「tomb raider」は movie ラベルも含んでいると言えます。）またその名詞句に信頼度が高いラベルが複数ある場合には、追加情報として文脈が考慮されるようになります。
    2. Microsoft Concept Graph
       また名詞句を分類するために、Microsoft Concept Graph も用いました。Google Knowledge Graph に比べて、より一般的なカテゴリが提供され、モジュールに分類する際に効率良く行うことができます。
    3. ASR Correction
       表現獲得のためにグラフを用いるというアイデアとは異なり、ASR 修正システムも用いました。詳細は後述されます。

    これらに加えて、表現抽出のために文脈も考慮に入れました。例えば、システムが提案した映画関連の質問を通すことによって（文脈の獲得）Table 1 に挙げる映画の話題で登場した「her」を検出することができます。後述されるように、これらから得られた情報は意図分類(intent classification) のために結合されます。
*** Coreference Resolution
    Stanford CoreNLP と NeuralCoref を用いた SOTA なモジュールは、非会話データを元にして訓練されていました。そのため会話での照応解消(anaphora resolution)（恐らく it, one などの意味的置換？）ではうまくいきませんでした。集められたデータを解析していくに連れ、我々は相互参照を受ける "more" や "one" といった単語をラベル付けしました。我々はこれらの単語をユーザの発話とシステムの応答を元に置換します。具体的には入力からの名詞句と、システムからの応答から得られる詳細な説明を含むNERの結果を、そのユーザの属性として保存します。そしてユーザが参照している内容に応じて、対応する相互参照の内容を提供します。
    将来的にはより多くの文脈を考慮できるようにし、我々は選択された単語リストを超えることができ、かつより明確に定義された優先順位で認識するようなモデルを訓練したいです。
*** ASR Correction
    ASR エラーは NLU の精度に大きな影響を与えることがわかっています。ASKは各単語の信頼度スコアと言語モデルから生成された信頼度スコアの両方を組み込むことで、全体的な ASR の信頼度スコアを算出します。全体のスコアは発話全体が正しく認識されている確率を示します。しかし誤検知（信頼度が低くなりASRエラーを発生してしまうこと）の可能性が2つあります。1つ目は言及されている単語が訓練データに頻繁に見られない際に、その単語の重みが低くなってしまうことです。もう一つは、ASRを用いることでも回避できない同音語による誤検知です。
    我々はユーザと knowledge base を用いて言及された名詞句 (但し頻度の高い単語を無視するものとする)を比較するため、double metaphone algorithm を用いました。knowledge base は文脈やドメイン（例えばスポーツのジャンルやゲーム名）情報を保存しています。詳しく言うと、値として単語をキーとして各単語のための double metaphone アルゴリズムによって得られる 1次コードと 2次コードを保存しました。更に観測に基づいて、特定のパターンを持つ単語に 3次コードを適用し、保存しました(例えば"jalapeno"という単語は、コードとして "HLPN", "JLPN" そして "ALPN" を持っています)。もしASRから得られる全体の信頼度が閾値(0.4 にセットされています)から低い場合には、名詞句の metaphone コードを knowledge base のそれと一致するような候補を調べ選択肢として提案します（妥当性のある文に書き換える？）。例えば、"obscure holuidays" についての話題が話されている際に、ユーザから "let's talk about secure holiday" という入力を受け取ったものとします。ここで "secure holiday" の １次コードは "SKRLT" であり、 "obscure holidays" の 1次コードは "APSKRLTS" です。ASRが発音の始まりと終わりをうまく認識できないことがあるため、この手法は比較的信頼度を高めることができると考えられます。別の例としては、スポーツについての話題の際に、"let's talk about he sport high ally" という入力がASRからあったとして、 high ally は一時コードして "jai alai" が knowledge base の "sports list" から提案されます。
*** Dialog Act Prediction
    NLU から得られたそれぞれの分割された文 (segmented sentence)は Dialog Act に関連付けられます。Dialog act は意見や主張といった会話の文脈を与えられた対話における機能です。この機能を予測するために、LSTMとCNN モデルを訓練しました。前者には 埋め込み次元300 の fastText 用いた単語埋め込みを用い、隠れ層の次元 500 の ２層の双方向LSTMモデルを採用しました。後者には同じく単語埋め込みを用いたカーネルサイズ３の２層CNNモデルを用いました。対話ログに十分なアノテーションこそないものの、我々は Switchboard Dialog Act Corpus (SWDA) が有用ではないかと考えました。 SWDA データセットはオープンドメインな電話対話の 205,000 発話を収録しており、60の dialog act のタグが付与されています。発話が順番にやってくる本システムのユースケースに対応するため、データを 156,809 発話に前処理し（carefully と強調しているので恐らく手作業？）、dialog act のタグを 40 まで減らしました。最も多いタグは statemenet-non-opinion や statement-opinion 、そして yes-no-questions でした。2つのモデルはこの処理されたデータセットに対して訓練されました。
    検証データに対する accuracy は LSTMモデルが 85.60% を達成したのに対して、CNNモデルは 85.25% を達成しました。更には、データセットとは無関係のデータ（143 のランダムに選ばれた独立な発話データ）で検証を行ったところ、LSTMモデルはCNNモデルよりも良い結果である 83.77% を達成しました。尚実際には、求めた dialog act のタグの種類は 19 にまでまとめられ、信頼度と共に求められます。例えば、 "awesome i like books why do you think the great gatsby is a great novel" は "awesome [appreciation] | i like books [opinion] | why do you think the great gatsby is great novel [open question]" という形に分割され (sentence segment の機能より)タグ付け(dialog act prediction)が行われます。誤りの過半数は statement/opinion の曖昧さによるもの（例えば "I like the movie Avengers"）、不完全な文が主な原因となっていると考えています。
    この Dialog act は文脈情報に依存しており、最適な結果を得るにはその前の対話内容及び現在の対話文（ segmented sentence）単位からの条件付き確率を最大化する必要があります。我々はこのモデルを事前訓練された埋め込みモデルであるELMoや再帰的な畳込みモデルを用いることで再帰化しようと考え、現在実験、評価を行っています。将来的には、dialog act prediction と sentence segmentation の両方を学習するような (multi-task learning) モデルを構築したいと考えています。更には、Dialog act prediction モデルと言語モデルを組み合わせて、発話が解釈不能であるかどうか、及びその応答がユーザに割り込み可能であるか（応答として適切であるかどうか）を判定したいと考えています。
*** Topic Expansion
    本システムでは抽出された名詞句の展開を行うための knowledge graph として ConceptNet を用いました。本システムの実装の初期段階に多く寄せられた意見として、システムがトピック（話題）を突然切り替えてしまったというものがありました。システム内のデータベースに保存できるようユーザに情報をより多く共有してもらう学習プロセスとは別に、本システムは同様のトピック（話題）について話すことができるようになりました。例えばユーザが車についての話題を話したい際に、本システムはConceptNetから様々なタイプの車（例えば Volvo）のリストを取得します。そのためユーザが好きな車の種類を尋ねとして、Dialog act によって分類された (e.g. story や opinion、asking a question) 入力文にコメントをつけた後、knowledge base からVolvoから拡張された情報を得ることができます。
** Dialog Management
   本システムはユーザの対話を扱うための2階層の対話管理を行います。上層ではNLUから得られる出力を活用して、それぞれのユーザの発話に対して最も適当な topic dialog module を選択します。下層では該当する応答を生成する dialog module を活性化させます。
*** High-Level System Dialog Management
    
