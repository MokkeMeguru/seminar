% Created 2019-05-14 Tue 15:08
% Intended LaTeX compiler: pdflatex
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{MokkeMeguru}
\date{\today \textit{<2019-04-24 Wed>}}
\title{モデルのパラメータ数を減少させる手法、ProdSumNet を読む}
\hypersetup{
 pdfauthor={MokkeMeguru},
 pdftitle={モデルのパラメータ数を減少させる手法、ProdSumNet を読む},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.2 (Org mode 9.2.2)}, 
 pdflang={Ja}}
\begin{document}

\maketitle
\tableofcontents

\section{導入}
\label{sec:org6440672}
ProdSumNet、というのは深層学習で \textbf{線形演算子} をより単純な形に書き換え、そのモデルの \textbf{*パラメータ数を減らす *} フレームワークを提案した論文です。この線形演算子はCNNや\href{https://arxiv.org/abs/1602.01407}{KFC}、\href{https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25}{Dilated CNN} といったアーキテクチャ全てにこの手法は適用可能であることが特徴として挙げられ、それらのパラメータ数を減少することに成功しました。実験としては、MNISTとFashion MNISTを用いました。

また別の内容としては、 \textbf{Convolutional Layer の代替となる手法} を提案しました。こいつは訓練することが出来るパラメータ数を動的に変更することができること、ややもすればパラメータ数と誤り率のトレードオフを調べることが出来ると考えられています。実験としてはMNIST を用い、 \(3 \times 10^6\) 以上の訓練パラメータを持つCNNネットワークを提案手法で置き換え、3554 個にまで訓練パラメータを減少させた上で 98.44\% の Accuracy を達成しました。
\section{モデルのパラメータ数を減少させると何が良いの？}
\label{sec:org5e5e61c}
　昨今の深層学習モデルは、とてもでっかいサイズになりがちな傾向が見られます。例えば昨年度から本年度頭にかけて、自然言語処理の分野ではBERTやGPT-2なんていうクソデカ言語モデルが台頭しています。こいつは学習時や推論時に、目が飛び出るような値段のGPUや一昔前のストレージサイズばりのメモリなんかのリソースをもりもり消費します。

　しかしそういった \textbf{強いけど重い深層学習モデル} というのは実用性があるのかと言われると、結構難しいところがあります。例えば (リアルタイム)音声合成 を行う機械学習モデルなんかはでかい計算機でビッグに計算してつよつよな精度が欲しいわけではなくて、寧ろモバイル機器でもとっととまともな出力が欲しい、という需要があります。(実際以前読んだRawNetなんかやFaceBookの研究している LPCNet なんかの研究成果は、精度について競う他に、音声合成の速度やリソースの使用量に視点をおいて研究が行われています。)

\section{関連研究はどんなものがあるの？}
\label{sec:org5db22c1}
　さて論文中ではモデルのパラメータ数を減少させる利点を２つ挙げ、その利点を得るための関連研究を挙げています。

　１つ目は、 \textbf{訓練後のモデルの軽量化} です。これは訓練後のモデルのパラメータを主成分分析なりSVDなりしてパラメータ数を圧縮する方法が考えられています。

　２つ目は、 \textbf{訓練中のモデル} の複雑さを軽減することです。そしてこれが本論文でフォーカスしている分野です。

　２つ目についてもう少し詳しく述べると、以下の式について考えることが出来ます。

\begin{eqnarray}
  y_{i+1} &=& f_i (W_i y_i + b_i)\ where\ i = 1, \dots, N \\
  where\ f_i &is& nonlinear\ function\ ex.\ ReLu\ or\ other\ identity\ function(R^{m_i}\rightarrow R^{n_{i+i}})\nonumber \\
         W_i &is& a\ matrix\ which\ means\ weight (R^{m_i\times n_i})\nonumber \\
         b_i &is& a\ vector\ which\ means\ bias (R^{m_i)}\nonumber \\
         y_i &is& a\ vector (R^{n_i})\nonumber
\end{eqnarray}

　この式は簡単な深層学習の一層を表しています。 \(f_i\) については事前に与えられており固定されたものとすると、この式の最適化は、 \(y_k\) への入力を \(x_k\) としてその際の正解出力を \(z_k\) 、予測出力を \(\tilde{z_k}\) とすると、パラメータ \(W_i, b_i\) に関する損失関数 \(d(\cdot, \cdot)\) について \(\Sigma_{k} d(\tilde{z_k}, z_k)\) の最小化とも言えます。

　しかしここで最適化しなければならないパラメータ(trainable parameter 訓練パラメータ)は一般に \(W_i, b_i\) のすべての要素の数 \(n_W\) で、これはとても大きな値であると言えます。\(n_W\) が大きいと、１反復にかかる計算が多くなること、 \sout{反復回数が多くなること(読んだ感想、を参照)} が予測され、つまり訓練時間が長くなってしまう可能性があります。

　つまり本論文では \textbf{この \(n_W\) を減らせるような普遍的な構造を提案したい} 、ということになります。

　この２つ目を達成する一般的手法としては、CNN に対して Convolutional layer を導入することです（なんのこっちゃと思いますが、そのままです [2]）。Convolutional Layer が持つ効果として、Shift invariant というものがあります。これはシフト不変性と訳されることもあるように、ちょっとズレた入力画像も同等に扱うことができるという機能を示しています(要：考察)。これによって訓練パラメータを劇的に減少させることが出来ます。これは shift invariant が必要となるような問題（例えば画像識別）ではうまく機能します。別の手法としては密行列(dense weight matrices)を巡回行列 (circulant matrices)に置き換えるものが知られています[3]。
  　
[2]: 原(The most well known model reduction technique is the introduction of the convolutional layer in convolutional neural networks.)

[3]: \href{https://arxiv.org/abs/1708.08917}{CirCNN} (Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian Tang, Qinru Qiu, Xue Lin, Bo Yuan)
\section{どういう手法を提案したの？}
\label{sec:org993b09f}
　この論文のアイデアの根幹は以下の式になります。

\begin{eqnarray}
W &=& \prod_{j=1}^{p}\Sigma_{k=1}^{s_i} g_{jk} (a_jk) M_{jk}\\
where\ a_{jk} &are&  the\ trainable\ parameters, also\ denoted\ as\ the\ variable\ parameters \nonumber \\
j &=& 1,\dots, p \nonumber \\
k &=& 1,\dots, s_j \nonumber \\
M_{jk} &are& the\ fixed\ parameters\ (matrices) \nonumber \\
g_{jk} &are& the\ fixed\ nonlinear\ functions \nonumber
\end{eqnarray}

　これは重み行列 \(W\) を計算するために提案された式です。

　 \(a_{jk}\) というのが訓練パラメータで、論文中では可変パラメータとも呼称されるものです。

　 \(M_{jk}\) は固定された行列であり、このサイズは、\(M_{jk}\) と \(M_{jk'}\) に関しては同じ次元数、 \(M_{jk}\) と \({M_{j+1, k}}\) に関してはこの間で加算、乗算が可能であるように制限されます。

　 非線形関数である \(g_{jk}\) は微分可能な関数であれば、簡単に導出することが出来るので、本論文中ではすべて \(g_{jk}(x) = x\) としています。

　この手法によって \(W\) のための訓練パラメータの総数 \(n_W\) は、 \(\Sigma_j {s_j}\) となります。つまり最適な重みである \(W_{opt}\) がもし \(a_{ij}\) を用いて近似分解できる場合(かつ \(\Sigma_{i} s_i\) が小さい場合)、この手法によってモデルのパラメータ数は劇的に減らすことが出来ます。またもし先に \(W_{opt}\) の近似が得られている場合（例えば転移学習など）では、パラメータ数を減らすためにその重み行列を上式の形に低ランク分解することが出来ます。

　またこの手法が convolutional layer や KFC, curculant matrices, Toeplitz matrices Handel matrices などでも有効であること(重み行列を分解することができること)は明らかです。

　但しこの手法は、 \(n_W\) を小さい値で抑えながら、最適な固定パラメータと分解を見つけることですが、本手法では訓練可能なパラメータを任意に変更できるという利点を持っています(他の手法を用いた分解方法だと、行列サイズを固定する必要があることが差異)。

　しかもこの提案手法は、現在ある深層学習用のアーキテクチャで用意に実装できます。この証拠として、先程の分解の式の偏微分は以下のように導出することが出来ます。



\section{どのくらいの精度が出たの？}
\label{sec:orge978423}
いくつかのテーマに別れて実験が行われているので、それぞれについて簡潔に紹介します。
\subsection{一つの行列和の分解(提案手法の p=1)}
\label{sec:org386fc18}

\subsection{複数の行列和の内積の分解(提案手法の p>1)}
\label{sec:org85f1c08}

\subsection{Convolutional Layer のない画像認識タスク}
\label{sec:orgfee8be9}

\section{読んだ感想とか}
\label{sec:org1ee2436}
　この論文、とてもおもしろい研究だと思うんですが、 \textbf{どこの学会にも出されていない} という不思議なことになっています。多分出したところから落とされたのかな？と思っているんですが、どうなんでしょう？

　また再現実験をしているレポジトリとかも見当たらないのが気になったりしています。 \sout{どこかに実装ないかな…}

　また、Pervasive Attention でわかったように、必ずしもパラメータ数が計算量に比例するわけではなさそうなので、この手法を用いることの利点とされる、訓練回数が少なくなる、というのは少し難しいと思われますね。
　
　（あとこの論文中で言われていた、CirCNNという論文がとてもパワーがあるのでそのうち目を通したいです。）
\end{document}
