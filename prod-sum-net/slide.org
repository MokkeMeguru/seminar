#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: モデルのパラメータ数を減少させる手法、ProdSumNet を読む
#+date: <2019-04-24 Wed>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 情報                                                             :noexport:

|---------------+----------------------------------+---------------------|
| First Author  | Chai Wah Wu　                    | IBM Research Member |
| Company       | IBM Research AI                  |                     |
|               | IBM T. J. Watson Research Center |                     |
|---------------+----------------------------------+---------------------|
| Second Author | nil                              |                     |
|---------------+----------------------------------+---------------------|
| Date          | September 6, 2018                |                     |
|---------------+----------------------------------+---------------------|
| Society       | nil                              |                     |
|---------------+----------------------------------+---------------------|
  
* 導入
  ProdSumNet、というのは深層学習で *線形演算子* をより単純な形に書き換え、そのモデルの **パラメータ数を減らす ** フレームワークを提案した論文です。この線形演算子はCNNやKFC(?)、[[https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25][Dilated CNN]] といったアーキテクチャ全てにこの手法は適用可能であることが特徴として挙げられ、それらのパラメータ数を減少することに成功しました。実験としては、MNISTとFashion MNISTを用いました。

  また別の内容としては、 *Convolutional Layer の代替となる手法* を提案しました。こいつは訓練することが出来るパラメータ数を動的に変更することができること、ややもすればパラメータ数と誤り率のトレードオフを調べることが出来ると考えられています。実験としてはMNIST を用い、 $3 \times 10^6$ 以上の訓練パラメータを持つCNNネットワークを提案手法で置き換え、3554 個にまで訓練パラメータを減少させた上で 98.44% の Accuracy を達成しました。
* モデルのパラメータ数を減少させると何が良いの？
  　昨今の深層学習モデルは、とてもでっかいサイズになりがちな傾向が見られます。例えば昨年度から本年度頭にかけて、自然言語処理の分野ではBERTやGPT-2なんていうクソデカ言語モデルが台頭しています。こいつは学習時や推論時に、目が飛び出るような値段のGPUや一昔前のストレージサイズばりのメモリなんかのリソースをもりもり消費します。

  　しかしそういった *強いけど重い深層学習モデル* というのは実用性があるのかと言われると、結構難しいところがあります。例えば (リアルタイム)音声合成 を行う機械学習モデルなんかはでかい計算機でビッグに計算してつよつよな精度が欲しいわけではなくて、寧ろモバイル機器でもとっととまともな出力が欲しい、という需要があります。(実際以前読んだRawNetなんかやFaceBookの研究している LPCNet なんかの研究成果は、精度について競う他に、音声合成の速度やリソースの使用量に視点をおいて研究が行われています。)

* 関連研究はどんなものがあるの？
  　さて論文中ではモデルのパラメータ数を減少させる利点を２つ挙げ、その利点を得るための関連研究を挙げています。

  　１つ目は、 *訓練後のモデルの軽量化* です。これは訓練後のモデルのパラメータを主成分分析なりSVDなりしてパラメータ数を圧縮する方法が考えられています。
  
  　２つ目は、 *訓練中のモデル* の複雑さを軽減することです。そしてこれが本論文でフォーカスしている分野です。

  　２つ目についてもう少し詳しく述べると、以下の式について考えることが出来ます。
  
  \begin{eqnarray}
    y_{i+1} &=& f_i (W_i y_i + b_i)\ where\ i = 1, \dots, N \\
    where\ f_i &is& nonlinear\ function\ ex.\ ReLu\ or\ other\ identity\ function(R^{m_i}\rightarrow R^{n_{i+i}})\nonumber \\
           W_i &is& a\ matrix\ which\ means\ weight (R^{m_i\times n_i})\nonumber \\
           b_i &is& a\ vector\ which\ means\ bias (R^{m_i)}\nonumber \\
           y_i &is& a\ vector (R^{n_i})\nonumber
  \end{eqnarray}
  
  　この式は簡単な深層学習の一層を表しています。 $f_i$ については事前に与えられており固定されたものとすると、この式の最適化は、 $y_k$ への入力を $x_k$ としてその際の正解出力を $z_k$ 、予測出力を $\tilde{z_k}$ とすると、パラメータ $W_i, b_i$ に関する損失関数 $d(\cdot, \cdot)$ について $\Sigma_{k} d(\tilde{z_k}, z_k)$ の最小化とも言えます。
  
  　しかしここで最適化しなければならないパラメータ(trainable parameter 訓練パラメータ)は一般に $W_i, b_i$ のすべての要素の数 $n_W$ で、これはとても大きな値であると言えます。$n_W$ が大きいと、１反復にかかる計算が多くなること、 +反復回数が多くなること(読んだ感想、を参照)+ が予測され、つまり訓練時間が長くなってしまう可能性があります。

  　つまり本論文では *この $n_W$ を減らせるような普遍的な構造を提案したい* 、ということになります。

  　この２つ目を達成する一般的手法としては、CNN に対して Convolutional layer を導入することです（なんのこっちゃと思いますが、そのままです [2]）。Convolutional Layer が持つ効果として、Shift invariant というものがあります。これはシフト不変性と訳されることもあるように、ちょっとズレた入力画像も同等に扱うことができるという機能を示しています(要：考察)。これによって訓練パラメータを劇的に減少させることが出来ます。これは shift invariant が必要となるような問題（例えば画像識別）ではうまく機能します。別の手法としては密行列(dense weight matrices)を巡回行列 (circulant matrices)に置き換えるものが知られています[3]。
  
  　
  [2]: 原(The most well known model reduction technique is the introduction of the convolutional layer in convolutional neural networks.)
  [3]: [[https://arxiv.org/abs/1708.08917][CirCNN]] (Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian Tang, Qinru Qiu, Xue Lin, Bo Yuan)
* どのくらいの精度が出たの？
  
* どういう手法を提案したの？
* 読んだ感想とか
  　この論文、とてもおもしろい研究だと思うんですが、 *どこの学会にも出されていない* という不思議なことになっています。多分出したところから落とされたのかな？と思っているんですが、どうなんでしょう？
  
  　また再現実験をしているレポジトリとかも見当たらないのが気になったりしています。 +どこかに実装ないかな…+

  　また、Pervasive Attention でわかったように、必ずしもパラメータ数が計算量に比例するわけではなさそうなので、この手法を用いることの利点とされる、訓練回数が少なくなる、というのは少し難しいと思われますね。
