#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: モデルのパラメータ数を減少させる手法、ProdSumNet を読む
#+date: <2019-04-24 Wed>
#+author: MokkeMeguru
#+email: meguru.mokke@gmail.com
#+language: ja
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 25.2.2 (Org mode 9.2.2)
* 情報                                                             :noexport:
|---------------+----------------------------------+---------------------|
| First Author  | Chai Wah Wu　                    | IBM Research Member |
| Company       | IBM Research AI                  |                     |
|               | IBM T. J. Watson Research Center |                     |
|---------------+----------------------------------+---------------------|
| Second Author | nil                              |                     |
|---------------+----------------------------------+---------------------|
| Date          | September 6, 2018                |                     |
|---------------+----------------------------------+---------------------|
| Society       | nil                              |                     |
|---------------+----------------------------------+---------------------|
  
* 導入
  ProdSumNet、というのは深層学習で *線形演算子* をより単純な形に書き換え、そのモデルの **パラメータ数を減らす ** フレームワークを提案した論文です。この線形演算子はCNNやKFC(?)、[[https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25][Dilated CNN]] といったアーキテクチャ全てにこの手法は適用可能であることが特徴として挙げられ、それらのパラメータ数を減少することに成功しました。実験としては、MNISTとFashion MNISTを用いました。

  また別の内容としては、 *Convolutional Layer の代替となる手法* を提案しました。こいつは訓練することが出来るパラメータ数を動的に変更することができること、ややもすればパラメータ数と誤り率のトレードオフを調べることが出来ると考えられています。実験としてはMNIST を用い、 $3 \times 10^6$ 以上の訓練パラメータを持つCNNネットワークを提案手法で置き換え、3554 個にまで訓練パラメータを減少させた上で 98.44% の Accuracy を達成しました。
* モデルのパラメータ数を減少させると何が良いの？
  　昨今の深層学習モデルは、とてもでっかいサイズになりがちな傾向が見られます。例えば昨年度から本年度頭にかけて、自然言語処理の分野ではBERTやGPT-2なんていうクソデカ言語モデルが台頭しています。こいつは学習時や推論時に、目が飛び出るような値段のGPUや一昔前のストレージサイズばりのメモリなんかのリソースをもりもり消費します。

  　しかしそういった *強いけど重い深層学習モデル* というのは実用性があるのかと言われると、結構難しいところがあります。例えば (リアルタイム)音声合成 を行う機械学習モデルなんかはでかい計算機でビッグに計算してつよつよな精度が欲しいわけではなくて、寧ろモバイル機器でもとっととまともな出力が欲しい、という需要があります。(実際以前読んだRawNetなんかやFaceBookの研究している LPCNet なんかの研究成果は、精度について競う他に、音声合成の速度やリソースの使用量に視点をおいて研究が行われています。)

* どのくらいの精度が出たの？
* どういう手法を提案したの？
* 読んだ感想とか
  　この論文、とてもおもしろい研究だと思うんですが、 *どこの学会にも出されていない* という不思議なことになっています。多分出したところから落とされたのかな？と思っているんですが、どうなんでしょう？
  
  　また再現実験をしているレポジトリとかも見当たらないのが気になったりしています。 =どこかに実装ないかな…=
