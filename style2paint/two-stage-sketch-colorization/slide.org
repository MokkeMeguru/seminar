#+TITLE: スケッチ to イラストな手法、style2paint を読む
#+AUTHOR: コンピュータサイエンス専攻 江畑 拓哉(201920631)
# This is a Bibtex reference
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:t arch:headline ^:nil
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:nil e:nil email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil title:t toc:nil todo:t |:t
#+LANGUAGE: ja
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 26.2 (Org mode 9.2.3)
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, dvipdfmx, 10pt]
#+LATEX_HEADER: \usepackage{amsmath, amssymb, bm}
#+LATEX_HEADER: \usepackage{graphics}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \usepackage{pxjahyper}
#+LATEX_HEADER: \hypersetup{colorlinks=false, pdfborder={0 0 0}}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[backend=biber, bibencoding=utf8]{biblatex}
#+LATEX_HEADER: \usepackage[top=20truemm, bottom=25truemm, left=25truemm, right=25truemm]{geometry}
#+LATEX_HEADER: \usepackage{ascmac}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \addbibresource{/home/meguru/Github/private-Journal/research-plan/reference.bib}
#+DESCRIPTION:
#+KEYWORDS:
#+STARTUP: indent overview inlineimages
* 導入
  #+BEGIN_QUOTE
  良い感じのお絵描きの補助ツールが欲しい！（友人談）
  #+END_QUOTE

  最近ですと、ClipStudio Paint Pro/Ex が *自動着彩機能* を beta 版で利用可能になるなど、 *お絵描き支援に関する研究* が *市場価値* を生んでいますね（学術的にどうかって？医療データでもない深層学習にお金が下りると思ってんの？）。
  
  そんなわけでスケッチtoイラストみたいな研究論文が海外や国内企業ではそれなりに数が出ているわけなんですが、その中でも即プロダクトに活かせる程度に高度なものとして [[https://github.com/lllyasviel/style2paints][style2paint]] というものがあります。これはラフ画からい感じに着彩するというまさに文脈通りの機能を実現するためのもので、実際に対話型のソフトウェアが構築されたものです。これはバージョンが存在しており、2019年6月時点で最新となっているのは v4 です。これに関する論文が、[[https://github.com/lllyasviel/style2paints/blob/master/papers/sa.pdf][Two-stage Sketch Colorization]] というものです。
  
  個人的な導入はともかくとして、簡単に論文で述べられている導入の方に踏み込んでみましょう。
  
  #+BEGIN_QUOTE
  着彩って難しくて時間かかるし、説得力のある絵を書くには、絵師でも苦労するよね。初心者の人にはお手本となるような、絵師さんには沢山の色の組み合わせを試せるような(例えば髪の色をちょっと変えてみたりとか)仕組みを作る必要があると思うんだ。
  
  でも着彩にの自動化をしようと思ったら結構多くの課題があって、例えば主題を制限していない状態でのスケッチ画をモデルに理解してもらうのは至難の業だし、スケッチ画は線画に比べれば大分抽象化されているからそこの差を埋めるようにしないといけない、テクスチャとかシェーディングみたいな情報も描かれていないからそれの推測もしなくちゃぁいけない。
  
  ところで最近だと深層学習の技術が発展してきて、[[https://nico-opendata.jp/ja/casestudy/comicolorization/index.html][Comicolorization]] なんかの技術が便利な着彩補助の関連研究として出てきているね。でもこいつらはにじみとか色の間違いとかが普通にあったりするんだ。これらを修正できるようにしなきゃいけないんだけど、スケッチ->着彩の間に僕たちユーザは上手いこと手を出すことが出来ていなくて、結局自動着彩の技術よりも手打ちの着彩の方がまだまだ人気という感じなんだ。
 
  そんなわけで僕たちは自動着彩じゃなくて半自動着彩のフレームワークを提案するよ。半自動というのはどういうことかっていうと、ユーザが着彩結果をちょいちょい弄って修正できるようにしているっていう意味、つまりスケッチ->着彩(1)->ユーザの修正->着彩(2)->...->着彩(マスター) みたいなことをしたいってことだね。
  提案したフレームワークは、２段階に別れたCNNベースのもので、一つ目は簡単に色をちらして色の概略図みたいなものを作るもの、もう一つはそこから間違いや変な質感になっているものなんかを修正して細かいところを詰めていくものになっているよ。繰り返しになるかもしれないけど、つまりスケッチｰ>着彩という問題をより単純な２つのタスクに分離したってことだね。これによってモデルの学習が効率化して、そのモデルが堅牢なものになったことがわかったよ。更に言うと、既存の着彩補助に関する手法で得られた着彩画を修正することもできることがわかったんだ。 

  それで現実世界でうまく適用できるか試すためにGUIアプリを作ってユーザに使ってもらったんだけど、この手法は2段階に分かれているとはいえ、ユーザが途中結果を見ながら修正ができることなんかも相まって、ユーザからの満足度は高かったよ。

  評価として用いた手法が、実アプリケーションを用いたユーザアンケートだよ。これによって既存手法より良いということを示しています。
  
  最後に簡単に僕達が出来たことをまとめるとこんな感じになるよ。
  
  1. 実世界で作られているスケッチから高品質な着彩ができる2段階の着彩モデルを作ったよ
  2. 僕たちの提案した手法は、既存の手法と比較して着彩の誤り修正が容易でより高品質な着彩結果を得ることができるようになったよ
  3. 実際にユーザに使ってもらえるように、実アプリケーションを作ってみたよ
  #+END_QUOTE
  
  ほぼほぼ全意訳ですが、導入でおおよそ重要なことを述べきっている論文なのでもりもり書いています。
  
  #+CAPTION: style2paint 実アプリケーション図
  [[./img/style2paint_app.PNG]]
  
* 関連研究
  沢山ありましたので、公開されているものか、読んでみて面白そうだったものか、実際に試して面白かったものだけを抜粋して簡潔に紹介します。
  
** 色伝搬
   スケッチ->単色で塗りつぶし

   - [[https://dcgi.fel.cvut.cz/home/sykorad/Sykora09-EG.pdf][Lazy Brush]]

     スケッチからいい感じに領域選択して色を塗りつぶす研究。深層学習とかの手法でもないし、恐らく機械学習というわけでもないです。[[http://animatetvp.blogspot.com/2015/01/lazybrush.html][実アプリケーションとしてある]]らしく、チュートリアルムービーを見ることが出来ますが、めっちゃ早いです。領域選択系になるので、医療データの癌領域特定なんかに活かせそうですね。大学かなんかでやればいいんじゃないっすかね。

** 機械学習ベースの自動着彩
   スケッチ->カラー画像 or グレースケール画像->カラー画像
   Scribbler と Comicolorization はざっくり言うと、encoder-decoder型のGenerator＋GANsのDiscriminatorのような形を取っています。この部分の研究だと、敵対性ネットワークを用いるのがデフォルトという感じになるようです。
   
   - [[https://arxiv.org/abs/1612.00835][Scribbler]]

     スケッチ から カラー画像 or 線画+補助色 から カラー画像を作る手法です。要所要所に色の情報を付け加えることで、より良い画像が出来る、というもので、本手法にはそれなりに近いのかな、と思いました。但しスケッチのクオリティが高すぎるので、初心者のお絵描き補助という目的に沿うことができるようには思えない感じです。
     #+CAPTION: Scribbler より引用
     [[./img/scribbler_abst.png]]

   - [[https://arxiv.org/pdf/1704.08834.pdf][Outline Colorization through Tandem Adversarial Networks.]]
     
     グレースケールの画像 から カラー画像を作るための手法です。色彩予測を行うネットワークと、シェーディングを行うネットワークを組み合わせて画像を作り出すネットワークです。グレースケール画像から色の予測を行い、その色予測と、元のグレースケール画像の陰影情報を組み合わせて画像を作る、というモデル(学習にはGANsのDiscriminatorを使う)で style2paint とは違った2段階モデルになっています。
    
   - [[https://arxiv.org/pdf/1705.01908.pdf][AutoPainter]]
     
     スケッチ から カラー画像を作るための手法です。GANsを用いた自動着彩について研究したいなら一度は読みたい、という感じに読みやすい論文です。（というよりは損失関数の定義がすごくわかりやすい形にな収まっている。）pix2pix とのみ比較しているのでどの程度の性能なのかイマイチ理解が出来ないところがあるが、少なくとも pix2pix に対しては圧勝しています。

     面白かったのでもう少し気になったところを書くと、損失関数に画像の滑らかさを付け足す項を追加している点で、それは以下のような式になります。

     $L_{tv} = \sqrt{(y_{i+1, j} - y_{i, j})^2 + (y_{i, j+1} - y_{i, j})}$ 
    
     この式は他の画像生成系の論文ではあんまり見ないものだったので(というよりくっきりした画像を作るのがGANsのVAEに対する強みの一つなので、それを潰しているようにも捉えられるということが不思議です)、面白みがあるなぁと思いました。

     ちなみに一時期 PaintChainer の論文の盗作なのでは？という議論が上がったりもしていましたが、これは恐らく間違いです。
     
   - PaintsChainer シリーズ

      スケッチ->カラー画像を作るための手法です。PFN の出した [[https://paintschainer.preferred.tech/index_ja.html][つよつよ成果物]] を引っさげたシリーズです。名前が、たんぽぽ->かな->さつき、となっている ~舐め腐った~ 特徴的なタイトルのものです。[[https://github.com/pfnet/PaintsChainer/issues/146][論文]]がないっぽいんですが、これはどういうこっちゃ…？

   - [[https://arxiv.org/abs/1706.06918][cGAN-based Manga Colorization Using a Single Training Image]]

     グレースケール漫画 から カラー漫画を作るための手法です。物凄い面白い手法を使っているんですが、簡単な特徴に関する説明は [[http://yusuke-ujitoko.hatenablog.com/entry/2017/07/01/234633][このページ]] にあります。大量のデータで殴りつける最近のビッグデータでグローバルなジャパニーズドリーム()なものとは違い、とても日本人臭い泥にまみれた手法を使っているので、一度読んでみると面白いと思います。
     
     ちなみにこの手法を用いて低賃金で鬼のように働かされている日本人の漫画家やアニメータを救おう！みたいな [[http://broncoscholar.library.cpp.edu/bitstream/handle/10211.3/207996/YanYiyang_Thesis2018.pdf?sequence=3][調査論文]] が *海外* で出ているのは、これも日本らしくて大好きです。

** 画像のスタイル変換
   画像のスタイル変換もスケッチ->カラー画像に使えるので関連研究として取り上げられています。
   
   - [[https://arxiv.org/abs/1711.09554][Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation]]
     
     GANsを用いた画像のスタイル変換に関する論文。教師あり学習。例えばセグメンテーション画像(オブジェクトごとに色分けされた画像…？)と写真のような画像との変換、線画から写真のような画像の変換、あるいはそれらの逆元が出来る、と主張されています。実装は [[https://github.com/godisboy/DRPAN][こちら]] から。DRPAN という GANs の応用みたいなモデルを使っているんですが、僕の低脳では理解できませんでした…
     
     #+CAPTION: 論文より引用
     [[./img/drp_abst.PNG]]

   - [[https://arxiv.org/abs/1605.09782][Adversarial Feature Learning]]
     
     教師なし学習。これはスタイル変換という文脈ではなく、双方向 GANs を求める研究であることに注目しました。最近ですと Flow-base のモデルが可逆な潜在表現獲得モデルとして有名ですが、GANsでもそのような試みが行われているという意味で非常に興味深かったです。GANs に関する数式がもりもりしているので、GANs の数式をたくさん見てみたい人なんかも読んでみると楽しいかもしれません。というかこの論文が読めれば GANs マスターってくらいには GANs を理解できると思います。

   - [[https://arxiv.org/pdf/1703.00848.pdf][Unsupervised Image-to-Image Translation Networks]]
     
     教師なし学習。実装は [[https://github.com/mingyuliutw/unit][こちら]] 。ドメインを2つ仮定して、それぞれのドメインにおける同義の意味を同じ潜在表現として取り扱うことでスタイル変換を行おうとしています。つまり $X_1$ のドメインからある画像 a と $X_2$ のドメインから a と同じシチュエーションなある画像 b について考えたときに、それぞれの潜在表現は同じ z ということになります。Generator や Discriminator はスタイルごとに必要になります。つまり $X_1$ のスタイルの画像についての Discriminator は、 $X_1$ から得られる画像か、 $X_2$ から得られた画像の潜在表現から $G_1$ を通して得られた $X_1$ のスタイルになった画像を判定するものになります。この論文をチョイスした理由は、自然言語含めスタイル変換全般に使えそうな手法だったからです。あとこれは後に拡張されて、2つのドメインからマルチドメインになったものが出てきていて、非常に [[https://github.com/NVlabs/MUNIT][興味深い論文]] だったからです([[https://github.com/NVlabs/MUNIT][実装]])。こっちの論文を読め（自分への圧力）。
     
     #+CAPTION: 論文より引用
     [[./img/uiit_abst.PNG]]

   - [[https://arxiv.org/abs/1703.10593][CycleGAN]]
     
     誰でも知っているので挙げました。解説は[[https://qiita.com/hikaru-light/items/98d06b21b4f3e2bb6ca4][このあたり]]で見てください。
     
** 画像の色付け
   - [[http://iizuka.cs.tsukuba.ac.jp/projects/colorization/ja/][Let there be Color!]]

     グレースケール画像 から カラー画像を作るための手法です。早稲田大学の出したグレースケール画像の自動着彩に関する論文。大域・中域・少域特徴を得るためのネットワーク＋色付けのネットワークの4つのネットワークをまとめ、彩色画像を作り、それを元のグレースケール画像と組み合わせることでカラー画像を生成します。テレビなんかでも大きく取り上げられたモデルらしいです。大域的・局所的、みたいな文言と最近出てきた [[https://qiita.com/koshian2/items/0e40a5930f1aa63a66b9][OctConv のモデル]] がなんとなく発想が似ている気がしたのでピックアップしました。
     
   - [[https://richzhang.github.io/ideepcolor/][Real-Time User-Guided Image Colorization with Learned Deep Priors]]
     
     グレースケール画像 から カラー画像を作るための手法です。着彩画像に修正が出来ることなど、ほぼほぼ style2paint と同じ仕様になっていますが、こちらは大体の位置に色を置く（塗るではない）することで着彩を行い、スケッチではなくグレースケール画像を入力に用います。かなり良い精度が出ており、これ、 *グリザイユ画法* で使えるんじゃね？と一人思っています。（数年くらい前から日本の一部コミュニティではグリザイユ画法が流行っているという *学術的に価値のない* モチベーションですね）ちなみに GANs のアイデアは使っているのに GANs の損失関数を使わないという面白い内容になっています。GANs を使わないでスタイル変換する論文をこの GANs 時代に提案してくるか…と関心しました。簡単な解説は [[https://github.com/DwangoMediaVillage/paper_readings/issues/8][ここ]] を読むと良いと思います。そして恐らくこれが最も本論文である style2paint に影響を与えていると思います（具体的には U-net 周りのアーキテクチャがかなり似通っています）。（ ~ただ見た目の精度が尋常じゃないのに評価手法がPSNRなのが結構気になります~ ）
     
     またこの論文では、ユーザの入力に対するシミュレーションも行っており、直接 style2paint のような手法に活かすにはやや大味ではあるものの、この手の手法のデータ収集に関して非常に参考になるものですので、 *一読するべき* でしょう(4ページの Simulating User Interactions. の部分です)
     
     #+CAPTION: 論文より引用
     [[./img/rtugi_abst.PNG]]

* モデル概要
論文では、提案手法の概要から2段階のステップそれぞれの構成、そして訓練データの作成手法についての説明がなされています。これらをざっくりと消化していきます。特に訓練データの作成・獲得手法については *pixiv のサーバダウンを狙ってスクレイピングアタック仕掛けている新進気鋭超頭脳AI研究者様* には見ていただきたいものですね。(~界隈や大学の印象悪くなるからやめてくれ~)

** OverView
2段階なフレームワークである本手法は、 *drafting stage* と *refinement stage* という名前で2つを区別しています。入力のスケッチと最初に与えられるユーザの指示を元に色の構成を決めて、ぱっと色付けをすることが drafting stage での目標になります。そして refinement stage では drafting stage での drafting stage で得られた画像について不正確な色の領域を識別して、追加のユーザからの指示群を元に改良します。これら2つの stage に対するモデルは別々に訓練されており、実際に検証を行う際に初めて接続され最終出力までを得ることが出来ます。以下の図 Fig. 3 がフレームワークの全体図です。この 2段階なフレームワークは複雑な着彩タスクをよりシンプルで目標が明確であるサブタスクに分割したことで、結果的にスケッチと着彩までの距離を狭めます。さらに学習が容易になり、着彩結果の品質が向上します。一方既存の1段階な着彩手法では学習が困難であるために、不自然な着彩に対する修正を行うことが出来ません。

訓練に際して *着彩済みなデータセット* として目をつけたものは [[https://www.gwern.net/Danbooru2018][Danbooru database]] でした。これに対するスケッチの獲得は、PaintsChainer による線画抽出システムを用いました。またユーザからの入力(指示)をシミュレートするには、[[https://arxiv.org/pdf/1705.02999.pdf][Real-Time User-Guided Image Colorization with LearnedDeep Priors.]] に用いられている手法を用いました。drafting と refinement 両方で用いられている本質的な手法は、 *GANs* です。Fig. 4 をみると、stacking layer と layer のサイズ、layer 間の接続方式についてわかると思います。訓練時にはおおよそ Adam Optimizerを用いています(where $\beta_1 = 0.9, \beta_2 = 0.99, lr=1e-5$)。訓練に用いた GPU は Tesla P100 で、バッチサイズは 16 でした(バッチサイズを上げると学習率を下げずに訓練がうまく行く、という論文を google が出していたはずなので、より強いGPU使って上げてみたいですね。)トレーニングのサンプルデータは、元画像から $224 \times 224$ のサイズのパッチにトリミングされます。とはいえ提案手法のモデルは [[https://esslab.jp/~ess/ja/research/sketch/][Fully Convolutional Network]] で構成されているので、本フレームワークの検証段階では *任意の入力サイズをサポートできる* ようになっています。

#+CAPTION: Fig.3 論文より引用
[[./img/s2p_fig3.PNG]]

#+CAPTION: Fig.4 論文より引用
[[./img/s2p_fig4.PNG]]
** drafting stage
この stage では入力データであるスケッチから大まかな全体の色構成を決定するという目的で学習されます。高品質な画像を求めているわけではなく、色の多様性を保証できるだけ、ユーザの指示に基づいた色を積極的に散らすことが出来る必要があります。このためにスケッチ $x$ と $u_i$ から大まかな画像 $\hat{y}_m$ を予測するネットワーク network G を提案しています。これの概要は Fig.4 (a)にあります。この大まかながぞうのせいせいについては PaintsChainer など他手法が存在していますが、これらは技術的詳細が明らかにされていません。しかし実験の結果、本手法はそれらと同等以上の性能(state-of-art な性能)が得られることがわかりました。

スケッチ $x$ とユーザの指示 $u_i$ を入力に、 $G(x, u_i)$ で表される FFN (feed-forward network) で 予測画像 $\hat{y}_m$ を出力します。最適化のための目的関数は次の式 (1) になります(概形は *1ノルム* と *色彩多様性確保のための補正項* 、そして *GANs* ですね)。

\begin{eqnarray}
  arg \min_{G} \max_{D} \mathbb{E}_{x, y_i, y_f \sim P_{data}(x, u_i, y_f)} [\|y_f - G(x, u_i)\|_{1} + \alpha L(G(x, u_i)) - \lambda log(D(y_f)) - \lambda log(1 - D(G(x, u_i)))] \\
where \nonumber \\
L(x) &=& - \Sigma^{3}_{c=1} \cfrac{1}{m} \Sigma^{m}_{i=1}(x_{c, i} - \cfrac{1}{m}\Sigma^{m}_{i=1}x_{c, i})^2 \\
x_{c, i} &=& the\ i-th\ element\ on\ the\ c-th\ cannel \nonumber \\
m &=& image\ width\ \times \ height \nonumber
\end{eqnarray}

損失関数 L では生成される *色彩のRGB空間における分散を高める効果* を担っており、これによってより *彩度の高い色をもった* 画像が生成できるようになります。

** refinement stage
drafting stage によって得られた画像はまだ色間違いや不自然な部分(英語でこれは artifact と言われます)があるため、実用的ではありません。これを修正するために、修正箇所の領域を特定し、それを修正します。このために本フレームワークではユーザから修正箇所の指摘を受けるという仕組みを取っており、その意図を汲み取り制御することが必要になります。これを他制するために、問題点のある色領域を特定・修正するための別の深層学習モデルを提案しました。このモデルはユーザの指示を集合として受け取り、

* 評価
* 感想
僕はあんまり画像系の研究はしていないんですが、この論文はとても読みやすい部類であったと思います。これも潜在空間をうまく使って研究しているような感じで、最近読んだ [[https://arxiv.org/abs/1904.09571][TransGaGa]] の技術を組み合わせるとか [[https://arxiv.org/abs/1807.03039][Glow]] や U-Net を参考に Fully Connected Network の構築手法をアップデートするとか、Refinement Stage を強化学習の分野に持ち込んでみる(本手法ではユーザの指示を集合として受け取ってしまっているがシーケンスで受け取ったほうがよりユーザの意図を汲み取れるのではないかという推察による)とか、多目的最適化問題として皆さん大好きな end2end な手法を目指してみるとか、色々研究していみたいテーマが見える面白い分野だなぁと思いました。（小並感）
  
  
  
  ClipStudioさん とか Pixiv さんとか PFN さんとかで研究してくれないかなぁ（チラッチラッ）
