#+TITLE: スケッチ to イラストな手法、style2paint を読む(1)
#+AUTHOR: コンピュータサイエンス専攻 江畑 拓哉(201920631)
# This is a Bibtex reference
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:t arch:headline ^:nil
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:nil e:nil email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil title:t toc:nil todo:t |:t
#+LANGUAGE: ja
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 26.2 (Org mode 9.2.3)
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, dvipdfmx, 10pt]
#+LATEX_HEADER: \usepackage{amsmath, amssymb, bm}
#+LATEX_HEADER: \usepackage{graphics}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \usepackage{pxjahyper}
#+LATEX_HEADER: \hypersetup{colorlinks=false, pdfborder={0 0 0}}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[backend=biber, bibencoding=utf8]{biblatex}
#+LATEX_HEADER: \usepackage[top=20truemm, bottom=25truemm, left=25truemm, right=25truemm]{geometry}
#+LATEX_HEADER: \usepackage{ascmac}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \addbibresource{/home/meguru/Github/private-Journal/research-plan/reference.bib}
#+DESCRIPTION:
#+KEYWORDS:
#+STARTUP: indent overview inlineimages
* 導入
  #+BEGIN_QUOTE
  良い感じのお絵描きの補助ツールが欲しい！（友人談）
  #+END_QUOTE

  最近ですと、ClipStudio Paint Pro/Ex が *自動着彩機能* を beta 版で利用可能になるなど、 *お絵描き支援に関する研究* が *市場価値* を生んでいますね（学術的にどうかって？医療データでもない深層学習にお金が下りると思ってんの？）。
  
  そんなわけでスケッチtoイラストみたいな研究論文が海外や国内企業ではそれなりに数が出ているわけなんですが、その中でも即プロダクトに活かせる程度に高度なものとして [[https://github.com/lllyasviel/style2paints][style2paint]] というものがあります。これはラフ画からい感じに着彩するというまさに文脈通りの機能を実現するためのもので、実際に対話型のソフトウェアが構築されたものです。これはバージョンが存在しており、2019年6月時点で最新となっているのは v4 です。これに関する論文が、[[https://github.com/lllyasviel/style2paints/blob/master/papers/sa.pdf][Two-stage Sketch Colorization]] というものです。
  
  個人的な導入はともかくとして、簡単に論文で述べられている導入の方に踏み込んでみましょう。
  
  #+BEGIN_QUOTE
  着彩って難しくて時間かかるし、説得力のある絵を書くには、絵師でも苦労するよね。初心者の人にはお手本となるような、絵師さんには沢山の色の組み合わせを試せるような(例えば髪の色をちょっと変えてみたりとか)仕組みを作る必要があると思うんだ。
  
  でも着彩にの自動化をしようと思ったら結構多くの課題があって、例えば主題を制限していない状態でのスケッチ画をモデルに理解してもらうのは至難の業だし、スケッチ画は線画に比べれば大分抽象化されているからそこの差を埋めるようにしないといけない、テクスチャとかシェーディングみたいな情報も描かれていないからそれの推測もしなくちゃぁいけない。
  
  ところで最近だと深層学習の技術が発展してきて、[[https://nico-opendata.jp/ja/casestudy/comicolorization/index.html][Comicolorization]] なんかの技術が便利な着彩補助の関連研究として出てきているね。でもこいつらはにじみとか色の間違いとかが普通にあったりするんだ。これらを修正できるようにしなきゃいけないんだけど、スケッチ->着彩の間に僕たちユーザは上手いこと手を出すことが出来ていなくて、結局自動着彩の技術よりも手打ちの着彩の方がまだまだ人気という感じなんだ。
 
  そんなわけで僕たちは自動着彩じゃなくて半自動着彩のフレームワークを提案するよ。半自動というのはどういうことかっていうと、ユーザが着彩結果をちょいちょい弄って修正できるようにしているっていう意味、つまりスケッチ->着彩(1)->ユーザの修正->着彩(2)->...->着彩(マスター) みたいなことをしたいってことだね。
  提案したフレームワークは、２段階に別れたCNNベースのもので、一つ目は簡単に色をちらして色の概略図みたいなものを作るもの、もう一つはそこから間違いや変な質感になっているものなんかを修正して細かいところを詰めていくものになっているよ。繰り返しになるかもしれないけど、つまりスケッチｰ>着彩という問題をより単純な２つのタスクに分離したってことだね。これによってモデルの学習が効率化して、そのモデルが堅牢なものになったことがわかったよ。更に言うと、既存の着彩補助に関する手法で得られた着彩画を修正することもできることがわかったんだ。 

  それで現実世界でうまく適用できるか試すためにGUIアプリを作ってユーザに使ってもらったんだけど、この手法は2段階に分かれているとはいえ、ユーザが途中結果を見ながら修正ができることなんかも相まって、ユーザからの満足度は高かったよ。

  評価として用いた手法が、実アプリケーションを用いたユーザアンケートだよ。これによって既存手法より良いということを示しています。
  
  最後に簡単に僕達が出来たことをまとめるとこんな感じになるよ。
  
  1. 実世界で作られているスケッチから高品質な着彩ができる2段階の着彩モデルを作ったよ
  2. 僕たちの提案した手法は、既存の手法と比較して着彩の誤り修正が容易でより高品質な着彩結果を得ることができるようになったよ
  3. 実際にユーザに使ってもらえるように、実アプリケーションを作ってみたよ
  #+END_QUOTE
  
  ほぼほぼ全意訳ですが、導入でおおよそ重要なことを述べきっている論文なのでもりもり書いています。
  
  #+CAPTION: style2paint 実アプリケーション図
  [[./img/style2paint_app.PNG]]
  
* 関連研究
  沢山ありましたので、公開されているものか、読んでみて面白そうだったものか、実際に試して面白かったものだけを抜粋して簡潔に紹介します。
  
** 色伝搬
   スケッチ->単色で塗りつぶし

   - [[https://dcgi.fel.cvut.cz/home/sykorad/Sykora09-EG.pdf][Lazy Brush]]

     スケッチからいい感じに領域選択して色を塗りつぶす研究。深層学習とかの手法でもないし、恐らく機械学習というわけでもないです。[[http://animatetvp.blogspot.com/2015/01/lazybrush.html][実アプリケーションとしてある]]らしく、チュートリアルムービーを見ることが出来ますが、めっちゃ早いです。領域選択系になるので、医療データの癌領域特定なんかに活かせそうですね。大学かなんかでやればいいんじゃないっすかね。

** 機械学習ベースの自動着彩
   スケッチ->カラー画像 or グレースケール画像->カラー画像
   Scribbler と Comicolorization はざっくり言うと、encoder-decoder型のGenerator＋GANsのDiscriminatorのような形を取っています。この部分の研究だと、敵対性ネットワークを用いるのがデフォルトという感じになるようです。
   
   - [[https://arxiv.org/abs/1612.00835][Scribbler]]

     スケッチ から カラー画像 or 線画+補助色 から カラー画像を作る手法です。要所要所に色の情報を付け加えることで、より良い画像が出来る、というもので、本手法にはそれなりに近いのかな、と思いました。但しスケッチのクオリティが高すぎるので、初心者のお絵描き補助という目的に沿うことができるようには思えない感じです。
     #+CAPTION: Scribbler より引用
     [[./img/scribbler_abst.png]]

   - [[https://arxiv.org/pdf/1704.08834.pdf][Outline Colorization through Tandem Adversarial Networks.]]
     
     グレースケールの画像 から カラー画像を作るための手法です。色彩予測を行うネットワークと、シェーディングを行うネットワークを組み合わせて画像を作り出すネットワークです。グレースケール画像から色の予測を行い、その色予測と、元のグレースケール画像の陰影情報を組み合わせて画像を作る、というモデル(学習にはGANsのDiscriminatorを使う)で style2paint とは違った2段階モデルになっています。
    
   - [[https://arxiv.org/pdf/1705.01908.pdf][AutoPainter]]
     
     スケッチ から カラー画像を作るための手法です。GANsを用いた自動着彩について研究したいなら一度は読みたい、という感じに読みやすい論文です。（というよりは損失関数の定義がすごくわかりやすい形にな収まっている。）pix2pix とのみ比較しているのでどの程度の性能なのかイマイチ理解が出来ないところがあるが、少なくとも pix2pix に対しては圧勝しています。

     面白かったのでもう少し気になったところを書くと、損失関数に画像の滑らかさを付け足す項を追加している点で、それは以下のような式になります。

     $L_{tv} = \sqrt{(y_{i+1, j} - y_{i, j})^2 + (y_{i, j+1} - y_{i, j})}$ 
    
     この式は他の画像生成系の論文ではあんまり見ないものだったので(というよりくっきりした画像を作るのがGANsのVAEに対する強みの一つなので、それを潰しているようにも捉えられるということが不思議です)、面白みがあるなぁと思いました。

     ちなみに一時期 PaintChainer の論文の盗作なのでは？という議論が上がったりもしていましたが、これは恐らく間違いです。
     
   - PaintsChainer シリーズ

      スケッチ->カラー画像を作るための手法です。PFN の出した [[https://paintschainer.preferred.tech/index_ja.html][つよつよ成果物]] を引っさげたシリーズです。名前が、たんぽぽ->かな->さつき、となっている ~舐め腐った~ 特徴的なタイトルのものです。[[https://github.com/pfnet/PaintsChainer/issues/146][論文]]がないっぽいんですが、これはどういうこっちゃ…？

   - [[https://arxiv.org/abs/1706.06918][cGAN-based Manga Colorization Using a Single Training Image]]

     グレースケール漫画 から カラー漫画を作るための手法です。物凄い面白い手法を使っているんですが、簡単な特徴に関する説明は [[http://yusuke-ujitoko.hatenablog.com/entry/2017/07/01/234633][このページ]] にあります。大量のデータで殴りつける最近のビッグデータでグローバルなジャパニーズドリーム()なものとは違い、とても日本人臭い泥にまみれた手法を使っているので、一度読んでみると面白いと思います。
     
     ちなみにこの手法を用いて低賃金で鬼のように働かされている日本人の漫画家やアニメータを救おう！みたいな [[http://broncoscholar.library.cpp.edu/bitstream/handle/10211.3/207996/YanYiyang_Thesis2018.pdf?sequence=3][調査論文]] が *海外* で出ているのは、これも日本らしくて大好きです。


** 画像のスタイル変換
   画像のスタイル変換もスケッチ->カラー画像に使えるので関連研究として取り上げられています。
   
   - [[https://arxiv.org/abs/1711.09554][Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation]]
     
     GANsを用いた画像のスタイル変換に関する論文。教師あり学習。例えばセグメンテーション画像(オブジェクトごとに色分けされた画像…？)と写真のような画像との変換、線画から写真のような画像の変換、あるいはそれらの逆元が出来る、と主張されています。実装は [[https://github.com/godisboy/DRPAN][こちら]] から。DRPAN という GANs の応用みたいなモデルを使っているんですが、僕の低脳では理解できませんでした…
     
     #+CAPTION: 論文より引用
     [[./img/drp_abst.PNG]]

   - [[https://arxiv.org/abs/1605.09782][Adversarial Feature Learning]]
     
     教師なし学習。これはスタイル変換という文脈ではなく、双方向 GANs を求める研究であることに注目しました。最近ですと Flow-base のモデルが可逆な潜在表現獲得モデルとして有名ですが、GANsでもそのような試みが行われているという意味で非常に興味深かったです。GANs に関する数式がもりもりしているので、GANs の数式をたくさん見てみたい人なんかも読んでみると楽しいかもしれません。というかこの論文が読めれば GANs マスターってくらいには GANs を理解できると思います。

   - [[https://arxiv.org/pdf/1703.00848.pdf][Unsupervised Image-to-Image Translation Networks]]
     
     教師なし学習。実装は [[https://github.com/mingyuliutw/unit][こちら]] 。ドメインを2つ仮定して、それぞれのドメインにおける同義の意味を同じ潜在表現として取り扱うことでスタイル変換を行おうとしています。つまり $X_1$ のドメインからある画像 a と $X_2$ のドメインから a と同じシチュエーションなある画像 b について考えたときに、それぞれの潜在表現は同じ z ということになります。Generator や Discriminator はスタイルごとに必要になります。つまり $X_1$ のスタイルの画像についての Discriminator は、 $X_1$ から得られる画像か、 $X_2$ から得られた画像の潜在表現から $G_1$ を通して得られた $X_1$ のスタイルになった画像を判定するものになります。この論文をチョイスした理由は、自然言語含めスタイル変換全般に使えそうな手法だったからです。あとこれは後に拡張されて、2つのドメインからマルチドメインになったものが出てきていて、非常に [[https://github.com/NVlabs/MUNIT][興味深い論文]] だったからです([[https://github.com/NVlabs/MUNIT][実装]])。こっちの論文を読め（自分への圧力）。
     
     #+CAPTION: 論文より引用
     [[./img/uiit_abst.PNG]]

   - [[https://arxiv.org/abs/1703.10593][CycleGAN]]
     
     誰でも知っているので挙げました。解説は[[https://qiita.com/hikaru-light/items/98d06b21b4f3e2bb6ca4][このあたり]]で見てください。
     
** 画像の色付け
   - [[http://iizuka.cs.tsukuba.ac.jp/projects/colorization/ja/][Let there be Color!]]

     グレースケール画像 から カラー画像を作るための手法です。早稲田大学の出したグレースケール画像の自動着彩に関する論文。大域・中域・少域特徴を得るためのネットワーク＋色付けのネットワークの4つのネットワークをまとめ、彩色画像を作り、それを元のグレースケール画像と組み合わせることでカラー画像を生成します。テレビなんかでも大きく取り上げられたモデルらしいです。大域的・局所的、みたいな文言と最近出てきた [[https://qiita.com/koshian2/items/0e40a5930f1aa63a66b9][OctConv のモデル]] がなんとなく発想が似ている気がしたのでピックアップしました。
     
   - [[https://richzhang.github.io/ideepcolor/][Real-Time User-Guided Image Colorization with Learned Deep Priors]]
     
     グレースケール画像 から カラー画像を作るための手法です。着彩画像に修正が出来ることなど、ほぼほぼ style2paint と同じ仕様になっていますが、こちらは大体の位置に色を置く（塗るではない）することで着彩を行い、スケッチではなくグレースケール画像を入力に用います。かなり良い精度が出ており、これ、 *グリザイユ画法* で使えるんじゃね？と一人思っています。（数年くらい前から日本の一部コミュニティではグリザイユ画法が流行っているという *学術的に価値のない* モチベーションですね）ちなみに GANs のアイデアは使っているのに GANs の損失関数を使わないという面白い内容になっています。GANs を使わないでスタイル変換する論文をこの GANs 時代に提案してくるか…と関心しました。簡単な解説は [[https://github.com/DwangoMediaVillage/paper_readings/issues/8][ここ]] を読むと良いと思います。そして恐らくこれが最も本論文である style2paint に影響を与えていると思います（具体的には U-net 周りのアーキテクチャがかなり似通っています）。（ ~ただ見た目の精度が尋常じゃないのに評価手法がPSNRなのが結構気になります~ ）
     
     またこの論文では、ユーザの入力に対するシミュレーションも行っており、直接 style2paint のような手法に活かすにはやや大味ではあるものの、この手の手法のデータ収集に関して非常に参考になるものですので、 *一読するべき* でしょう(4ページの Simulating User Interactions. の部分です)
     
     #+CAPTION: 論文より引用
     [[./img/rtugi_abst.PNG]]

* モデル概要
** Stage 1
** Stage 2
* 評価
* 感想
  僕はあんまり画像系の研究はしていないんですが、この論文はとても読みやすい部類であったと思います。これも潜在空間をうまく使って研究しているような感じで、最近読んだ [TransGaGa][TransGaGa] の技術を組み合わせるとか [Glow][Glow] を参考に潜在表現の構築手法をアップデートするとか、Second Stage を強化学習の分野に持ち込んでみるとか、色々研究していみたいテーマが見える面白い分野だなぁと思いました。（小並感・２分位でぱっと提案した感）
  
  
  
  ClipStudioさん とか Pixiv さんとか PFN さんとかで研究してくれないかなぁ（チラッチラッ）
